{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # https://github.com/DLR-RM/stable-baselines3/pull/780\n",
    "# !pip install gymnasium\n",
    "# !pip install 'gymnasium[mujoco]'\n",
    "# !pip install matplotlib\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install \"sb3_contrib>=2.0.0a1\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import gymnasium as gym\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,env_name:str, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, f\"{timestr}_{env_name}\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -14.55\n",
      "Saving new best model to ./tmp/gym/20230417-224635_Ant-v4.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -198.07\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -120.43\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -98.88\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -86.56\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -104.01\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -112.26\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -103.50\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -113.41\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -119.85\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -111.31\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -102.30\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -101.23\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -108.67\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -110.04\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -120.24\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -130.39\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -139.97\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -118.48\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -125.01\n",
      "Num timesteps: 21000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -124.21\n",
      "Num timesteps: 22000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -115.92\n",
      "Num timesteps: 23000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -126.24\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -123.17\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -124.41\n",
      "Num timesteps: 26000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -138.61\n",
      "Num timesteps: 27000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -148.46\n",
      "Num timesteps: 28000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -158.65\n",
      "Num timesteps: 29000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -149.32\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -156.47\n",
      "Num timesteps: 31000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -163.23\n",
      "Num timesteps: 32000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -162.31\n",
      "Num timesteps: 33000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -151.21\n",
      "Num timesteps: 34000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -149.87\n",
      "Num timesteps: 35000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -151.63\n",
      "Num timesteps: 36000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -151.20\n",
      "Num timesteps: 37000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -152.34\n",
      "Num timesteps: 38000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -154.03\n",
      "Num timesteps: 39000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -145.15\n",
      "Num timesteps: 40000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -142.28\n",
      "Num timesteps: 41000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -151.72\n",
      "Num timesteps: 42000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -159.33\n",
      "Num timesteps: 43000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -121.54\n",
      "Num timesteps: 44000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -115.83\n",
      "Num timesteps: 45000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -113.41\n",
      "Num timesteps: 46000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -97.26\n",
      "Num timesteps: 47000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -105.39\n",
      "Num timesteps: 48000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -82.10\n",
      "Num timesteps: 49000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -94.01\n",
      "Num timesteps: 50000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -73.58\n",
      "Num timesteps: 51000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -83.12\n",
      "Num timesteps: 52000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -82.59\n",
      "Num timesteps: 53000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -70.80\n",
      "Num timesteps: 54000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -69.62\n",
      "Num timesteps: 55000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -67.46\n",
      "Num timesteps: 56000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -77.52\n",
      "Num timesteps: 57000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -68.06\n",
      "Num timesteps: 58000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -73.32\n",
      "Num timesteps: 59000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -66.11\n",
      "Num timesteps: 60000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -64.99\n",
      "Num timesteps: 61000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -75.16\n",
      "Num timesteps: 62000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -70.60\n",
      "Num timesteps: 63000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -70.70\n",
      "Num timesteps: 64000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -71.16\n",
      "Num timesteps: 65000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -69.27\n",
      "Num timesteps: 66000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 67000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -88.07\n",
      "Num timesteps: 68000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -89.56\n",
      "Num timesteps: 69000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -97.05\n",
      "Num timesteps: 70000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -106.47\n",
      "Num timesteps: 71000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -111.94\n",
      "Num timesteps: 72000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -104.01\n",
      "Num timesteps: 73000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -113.17\n",
      "Num timesteps: 74000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -101.97\n",
      "Num timesteps: 75000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -110.99\n",
      "Num timesteps: 76000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -110.42\n",
      "Num timesteps: 77000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -112.75\n",
      "Num timesteps: 78000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -112.12\n",
      "Num timesteps: 79000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -106.31\n",
      "Num timesteps: 80000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -106.20\n",
      "Num timesteps: 81000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -95.59\n",
      "Num timesteps: 82000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -96.62\n",
      "Num timesteps: 83000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -94.88\n",
      "Num timesteps: 84000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -104.49\n",
      "Num timesteps: 85000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -112.02\n",
      "Num timesteps: 86000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -111.91\n",
      "Num timesteps: 87000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -114.23\n",
      "Num timesteps: 88000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -103.35\n",
      "Num timesteps: 89000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -95.47\n",
      "Num timesteps: 90000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -103.64\n",
      "Num timesteps: 91000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -111.68\n",
      "Num timesteps: 92000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -104.98\n",
      "Num timesteps: 93000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -105.11\n",
      "Num timesteps: 94000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -104.28\n",
      "Num timesteps: 95000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -107.70\n",
      "Num timesteps: 96000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -100.16\n",
      "Num timesteps: 97000\n",
      "Best mean reward: -14.55 - Last mean reward per episode: -87.72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env_name = \"Ant-v4\"\n",
    "env = gym.make(env_name,render_mode='human')\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from torch import nn\n",
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(env_name=env_name,check_freq=1000, log_dir=log_dir)\n",
    "\n",
    "# Ant-v3:\n",
    "#   normalize: true\n",
    "#   n_envs: 1\n",
    "#   policy: 'MlpPolicy'\n",
    "#   n_timesteps: !!float 1e7\n",
    "#   batch_size: 32\n",
    "#   n_steps: 512\n",
    "#   gamma: 0.98\n",
    "#   learning_rate: 1.90609e-05\n",
    "#   ent_coef: 4.9646e-07\n",
    "#   clip_range: 0.1\n",
    "#   n_epochs: 10\n",
    "#   gae_lambda: 0.8\n",
    "#   max_grad_norm: 0.6\n",
    "#   vf_coef: 0.677239\n",
    "model = PPO(\"MlpPolicy\", env,batch_size=32,n_steps=512,gamma=0.98,learning_rate=1.90609e-05,ent_coef=4.9646e-07,clip_range=0.1,n_epochs=10,gae_lambda=0.8,max_grad_norm=0.6,vf_coef=0.677239)\n",
    "model.learn(total_timesteps=1e7,callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
