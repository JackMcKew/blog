{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # https://github.com/DLR-RM/stable-baselines3/pull/780\n",
    "# !pip install gymnasium\n",
    "# !pip install 'gymnasium[mujoco]'\n",
    "# !pip install matplotlib\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install \"sb3_contrib>=2.0.0a1\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,env_name:str, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, f\"{timestr}_{env_name}\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env_name = \"Ant-v4\"\n",
    "env = gym.make(env_name,render_mode='human')\n",
    "env = Monitor(env, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Ant-v3:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m#   normalize: true\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#   n_envs: 1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m#   max_grad_norm: 0.6\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#   vf_coef: 0.677239\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env,batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,n_steps\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m,gamma\u001b[39m=\u001b[39m\u001b[39m0.98\u001b[39m,learning_rate\u001b[39m=\u001b[39m\u001b[39m1.90609e-05\u001b[39m,ent_coef\u001b[39m=\u001b[39m\u001b[39m4.9646e-07\u001b[39m,clip_range\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,n_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,gae_lambda\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m,max_grad_norm\u001b[39m=\u001b[39m\u001b[39m0.6\u001b[39m,vf_coef\u001b[39m=\u001b[39m\u001b[39m0.677239\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1e7\u001b[39;49m,callback\u001b[39m=\u001b[39;49mcallback)\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:171\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:57\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 57\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     58\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     59\u001b[0m         )\n\u001b[1;32m     60\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     61\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/gymnasium/envs/mujoco/ant_v4.py:349\u001b[0m, in \u001b[0;36mAntEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    346\u001b[0m reward \u001b[39m=\u001b[39m rewards \u001b[39m-\u001b[39m costs\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    350\u001b[0m \u001b[39mreturn\u001b[39;00m observation, reward, terminated, \u001b[39mFalse\u001b[39;00m, info\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:379\u001b[0m, in \u001b[0;36mMujocoEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 379\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmujoco_renderer\u001b[39m.\u001b[39;49mrender(\n\u001b[1;32m    380\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcamera_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcamera_name\n\u001b[1;32m    381\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:673\u001b[0m, in \u001b[0;36mMujocoRenderer.render\u001b[0;34m(self, render_mode, camera_id, camera_name)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n\u001b[1;32m    672\u001b[0m \u001b[39melif\u001b[39;00m render_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 673\u001b[0m     \u001b[39mreturn\u001b[39;00m viewer\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:413\u001b[0m, in \u001b[0;36mWindowViewer.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    412\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 413\u001b[0m         update()\n\u001b[1;32m    414\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    416\u001b[0m \u001b[39m# clear overlay\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:394\u001b[0m, in \u001b[0;36mWindowViewer.render.<locals>.update\u001b[0;34m()\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[39mfor\u001b[39;00m gridpos, [t1, t2] \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_overlays\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    385\u001b[0m         mujoco\u001b[39m.\u001b[39mmjr_overlay(\n\u001b[1;32m    386\u001b[0m             mujoco\u001b[39m.\u001b[39mmjtFontScale\u001b[39m.\u001b[39mmjFONTSCALE_150,\n\u001b[1;32m    387\u001b[0m             gridpos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcon,\n\u001b[1;32m    392\u001b[0m         )\n\u001b[0;32m--> 394\u001b[0m glfw\u001b[39m.\u001b[39;49mswap_buffers(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow)\n\u001b[1;32m    395\u001b[0m glfw\u001b[39m.\u001b[39mpoll_events()\n\u001b[1;32m    396\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_per_render \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_per_render \u001b[39m+\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m (\n\u001b[1;32m    397\u001b[0m     time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m render_start\n\u001b[1;32m    398\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/glfw/__init__.py:2377\u001b[0m, in \u001b[0;36mswap_buffers\u001b[0;34m(window)\u001b[0m\n\u001b[1;32m   2370\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mswap_buffers\u001b[39m(window):\n\u001b[1;32m   2371\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m \u001b[39m    Swaps the front and back buffers of the specified window.\u001b[39;00m\n\u001b[1;32m   2373\u001b[0m \n\u001b[1;32m   2374\u001b[0m \u001b[39m    Wrapper for:\u001b[39;00m\n\u001b[1;32m   2375\u001b[0m \u001b[39m        void glfwSwapBuffers(GLFWwindow* window);\u001b[39;00m\n\u001b[1;32m   2376\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2377\u001b[0m     _glfw\u001b[39m.\u001b[39;49mglfwSwapBuffers(window)\n",
      "File \u001b[0;32m~/miniconda3/envs/d3/lib/python3.11/site-packages/glfw/__init__.py:686\u001b[0m, in \u001b[0;36m_prepare_errcheck.<locals>.errcheck\u001b[0;34m(result, *args)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_errcheck\u001b[39m():\n\u001b[1;32m    679\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m    This function sets the errcheck attribute of all ctypes wrapped functions\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39m    to evaluate the _exc_info_from_callback global variable and re-raise any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39m    using the _callback_exception_decorator.\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39merrcheck\u001b[39m(result, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    687\u001b[0m         \u001b[39mglobal\u001b[39;00m _exc_info_from_callback\n\u001b[1;32m    688\u001b[0m         \u001b[39mif\u001b[39;00m _exc_info_from_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(env_name=env_name,check_freq=1000, log_dir=log_dir)\n",
    "\n",
    "# Ant-v3:\n",
    "#   normalize: true\n",
    "#   n_envs: 1\n",
    "#   policy: 'MlpPolicy'\n",
    "#   n_timesteps: !!float 1e7\n",
    "#   batch_size: 32\n",
    "#   n_steps: 512\n",
    "#   gamma: 0.98\n",
    "#   learning_rate: 1.90609e-05\n",
    "#   ent_coef: 4.9646e-07\n",
    "#   clip_range: 0.1\n",
    "#   n_epochs: 10\n",
    "#   gae_lambda: 0.8\n",
    "#   max_grad_norm: 0.6\n",
    "#   vf_coef: 0.677239\n",
    "model = PPO(\"MlpPolicy\", env,batch_size=32,n_steps=512,gamma=0.98,learning_rate=1.90609e-05,ent_coef=4.9646e-07,clip_range=0.1,n_epochs=10,gae_lambda=0.8,max_grad_norm=0.6,vf_coef=0.677239)\n",
    "model.learn(total_timesteps=1e7,callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "# NOTE: if you have loading issue, you can pass `print_system_info=True`\n",
    "# to compare the system on which the model was trained vs the current one\n",
    "# model = DQN.load(\"dqn_lunar\", env=env, print_system_info=True)\n",
    "env = gym.make(env_name,render_mode='human')\n",
    "model = PPO.load(\"./tmp/gym/20230417-224635_Ant-v4.zip\", env=env)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
