{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco\n",
    "import mujoco_viewer\n",
    "\n",
    "model = mujoco.MjModel.from_xml_path('humanoid.xml')\n",
    "data = mujoco.MjData(model)\n",
    "\n",
    "# create the viewer object\n",
    "viewer = mujoco_viewer.MujocoViewer(model, data)\n",
    "\n",
    "# simulate and render\n",
    "for _ in range(10000):\n",
    "    if viewer.is_alive:\n",
    "        mujoco.mj_step(model, data)\n",
    "        viewer.render()\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# close\n",
    "viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "import mujoco\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_random_seed(0)\n",
    "\n",
    "# Define the Mujoco environment\n",
    "class MujocoEnv(gym.Env):\n",
    "    def __init__(self, xml_path):\n",
    "        self.model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "        self.data = mujoco.MjData(self.model)\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(self.model.nu,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self.model.nq + self.model.nv,))\n",
    "        self.viewer_setup = False\n",
    "\n",
    "    def step(self, action):\n",
    "        self.data.ctrl[:] = action\n",
    "        self.step()\n",
    "        obs = np.concatenate([self.sim.data.qpos, self.sim.data.qvel])\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        info = {}\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        mjlib.mj_resetData(self.model.ptr, self.sim.data.ptr)\n",
    "        qpos = self.model.data.qpos\n",
    "        qpos[0] = 1.0\n",
    "        qpos[2] = 0.2\n",
    "        qpos[3] = 0.0\n",
    "        qpos[4] = 0.0\n",
    "        qpos[5] = 0.0\n",
    "        qpos[6] = 0.0\n",
    "        qpos[7] = 0.0\n",
    "        qpos[8] = 0.0\n",
    "        qpos[9] = 0.0\n",
    "        qpos[10] = 0.0\n",
    "        qpos[11] = 0.0\n",
    "        qpos[12] = 0.0\n",
    "        self.sim.data.qpos[:] = qpos\n",
    "        self.sim.forward()\n",
    "        obs = np.concatenate([self.sim.data.qpos, self.sim.data.qvel])\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if not self.viewer_setup:\n",
    "            self.viewer_setup = True\n",
    "            self.viewer.cam.distance = 3.0\n",
    "            self.viewer.cam.elevation = -20.0\n",
    "            self.viewer.cam.lookat[0] = 0.0\n",
    "            self.viewer.cam.lookat[1] = 0.0\n",
    "            self.viewer.cam.lookat[2] = 0.5\n",
    "        self.viewer.render()\n",
    "\n",
    "# Create a vectorized environment\n",
    "env = SubprocVecEnv([lambda: MujocoEnv('humanoid.xml') for i in range(1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train the agent using PPO\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# # Save the trained model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# model.save(\"trained_model\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[39m# Evaluate the trained agent\u001b[39;00m\n\u001b[1;32m      9\u001b[0m obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/miniconda3/envs/gym/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:304\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    293\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    302\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(PPO, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    305\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    306\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    307\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    308\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    309\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    310\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    311\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    312\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gym/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:242\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    229\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    230\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    239\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOnPolicyAlgorithm\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    240\u001b[0m     iteration \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 242\u001b[0m     total_timesteps, callback \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_learn(\n\u001b[1;32m    243\u001b[0m         total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, eval_log_path, reset_num_timesteps, tb_log_name\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    246\u001b[0m     callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    248\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n",
      "File \u001b[0;32m~/miniconda3/envs/gym/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:429\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39m# Avoid resetting the environment when calling ``.learn()`` consecutive times\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39mif\u001b[39;00m reset_num_timesteps \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset()  \u001b[39m# pytype: disable=annotation-type-mismatch\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m    431\u001b[0m     \u001b[39m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gym/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:132\u001b[0m, in \u001b[0;36mSubprocVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvObs:\n\u001b[1;32m    131\u001b[0m     \u001b[39mfor\u001b[39;00m remote \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremotes:\n\u001b[0;32m--> 132\u001b[0m         remote\u001b[39m.\u001b[39;49msend((\u001b[39m\"\u001b[39;49m\u001b[39mreset\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m))\n\u001b[1;32m    133\u001b[0m     obs \u001b[39m=\u001b[39m [remote\u001b[39m.\u001b[39mrecv() \u001b[39mfor\u001b[39;00m remote \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremotes]\n\u001b[1;32m    134\u001b[0m     \u001b[39mreturn\u001b[39;00m _flatten_obs(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space)\n",
      "File \u001b[0;32m~/miniconda3/envs/gym/lib/python3.10/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_bytes(_ForkingPickler\u001b[39m.\u001b[39;49mdumps(obj))\n",
      "File \u001b[0;32m~/miniconda3/envs/gym/lib/python3.10/multiprocessing/connection.py:411\u001b[0m, in \u001b[0;36mConnection._send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send(buf)\n\u001b[1;32m    406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[39m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[39m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[39m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[39m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send(header \u001b[39m+\u001b[39;49m buf)\n",
      "File \u001b[0;32m~/miniconda3/envs/gym/lib/python3.10/multiprocessing/connection.py:368\u001b[0m, in \u001b[0;36mConnection._send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m remaining \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(buf)\n\u001b[1;32m    367\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     n \u001b[39m=\u001b[39m write(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, buf)\n\u001b[1;32m    369\u001b[0m     remaining \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n\n\u001b[1;32m    370\u001b[0m     \u001b[39mif\u001b[39;00m remaining \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "# Train the agent using PPO\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save(\"trained_model\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
