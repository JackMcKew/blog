{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('.env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "6283ec5bfe405835c0d3dc0e40ab7fee6b8e24f99d68385023de9daea29edfe6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Now that we've built & trained logistic regression and decision tree models to classify the iris dataset in these previous posts:\n",
    "\n",
    "- [LINK TO LOGISTIC REGRESSION POST]\n",
    "- [LINK TO DECISION TREE POST]\n",
    "\n",
    "We found that they were both really good in their own regard (potentially overfitting), but what if we had two models that had pros/cons of each but we wanted the best of both worlds? In machine learning such a thing exists and it's known as ensemble models where you combine multiple models together to make a single model with hopefully the strengths of each of the models that are combined. There are many methods to how we combine them together which are grouped under 2 main categories: averaging and boosting.\n",
    "\n",
    "Averaging ensemble methods is when we build multiple models independantly and then average out their predictions. By doing this, the variance of the model is reduced and typically increases the performance of the model. Boosting ensemble methods is when we build models sequentially where each model depends on the previous and combine them in a specific strategy for the final model.\n",
    "\n",
    "For this post, we'll use `sklearn` to train each of the models that we previously trained and combine them together to see how they fair against each other.\n",
    "\n",
    "As always we begin by loading the data.\n",
    "\n",
    "> Best practice would be to split the training and testing data here, but for brevity we will skip this step."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "source": [
    "Next we will train both our decision tree model and our logistic regression model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionTreeClassifier = DecisionTreeClassifier().fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logisiticRegression = LogisticRegression().fit(iris.data, iris.target)"
   ]
  },
  {
   "source": [
    "Now let's create a function that we can pass a model into that will give us a report on the score for the model, so then we can compare how well each model is performing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_model_score(model, model_name):\n",
    "    predictions = model.predict(iris.data)\n",
    "\n",
    "    print(f\"{model_name}\")\n",
    "    print(classification_report(iris.target, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "logisiticRegression\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        50\n           1       0.98      0.94      0.96        50\n           2       0.94      0.98      0.96        50\n\n    accuracy                           0.97       150\n   macro avg       0.97      0.97      0.97       150\nweighted avg       0.97      0.97      0.97       150\n\ndecisionTreeClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        50\n           1       1.00      1.00      1.00        50\n           2       1.00      1.00      1.00        50\n\n    accuracy                           1.00       150\n   macro avg       1.00      1.00      1.00       150\nweighted avg       1.00      1.00      1.00       150\n\n"
     ]
    }
   ],
   "source": [
    "get_model_score(logisiticRegression, 'logisiticRegression')\n",
    "get_model_score(decisionTreeClassifier, 'decisionTreeClassifier')"
   ]
  },
  {
   "source": [
    "As seen from the previous posts, the models are very strong in their own regard but we make sure to note that given the sample size we are potentially overfitting drastically to the dataset.\n",
    "\n",
    "Since we will be making use of the voting classifier method later on, which takes a majority vote of the outcome of the models, we will need an odd number of models to give a worthwhile comparison, so let's train another model using k-Nearest Neighbours.\n",
    "\n",
    "Now this model is a bit different in that we search for the best parameters for the model and then select the best model of this specific type that will go into our future ensemble. The `cv` argument stands for cross validation, which means the dataset is randomly split into a number of groups, similar to how we train/test split our dataset.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KNearestNeighboursClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        50\n           1       0.96      0.96      0.96        50\n           2       0.96      0.96      0.96        50\n\n    accuracy                           0.97       150\n   macro avg       0.97      0.97      0.97       150\nweighted avg       0.97      0.97      0.97       150\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn_params = {'n_neighbors': np.arange(1,50)}\n",
    "\n",
    "knn_grid_search = GridSearchCV(knn, knn_params, cv=5)\n",
    "\n",
    "knn_grid_search.fit(iris.data, iris.target)\n",
    "\n",
    "knn_best = knn_grid_search.best_estimator_\n",
    "\n",
    "get_model_score(knn_best, 'KNearestNeighboursClassifier')"
   ]
  },
  {
   "source": [
    "\n",
    "Now let's combine all of these models into a single ensemble model using the voting classifier method, this takes the majority of the models to decide on the output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ensembleClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        50\n           1       0.98      0.98      0.98        50\n           2       0.98      0.98      0.98        50\n\n    accuracy                           0.99       150\n   macro avg       0.99      0.99      0.99       150\nweighted avg       0.99      0.99      0.99       150\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "models = [\n",
    "    ('logisiticRegression', logisiticRegression),\n",
    "    ('decisionTreeClassifier', decisionTreeClassifier),\n",
    "    ('kNearestNeighboursClassifier', knn_best)\n",
    "]\n",
    "\n",
    "ensembleClassifier = VotingClassifier(models, voting='hard')\n",
    "\n",
    "ensembleClassifier.fit(iris.data, iris.target)\n",
    "\n",
    "get_model_score(ensembleClassifier, 'ensembleClassifier')"
   ]
  },
  {
   "source": [
    "Now while our training dataset is small in this scenario, it's likely that the ensemble model will outperform the others due to it's lower variance and it being more adapted to multiple scenarios."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}