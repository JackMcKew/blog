{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from IPython.core.display import HTML\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   sepal.length  sepal.width  petal.length  petal.width variety\n0           5.1          3.5           1.4          0.2  Setosa\n1           4.9          3.0           1.4          0.2  Setosa\n2           4.7          3.2           1.3          0.2  Setosa\n3           4.6          3.1           1.5          0.2  Setosa\n4           5.0          3.6           1.4          0.2  Setosa\nVirginica     50\nSetosa        50\nVersicolor    50\nName: variety, dtype: int64\n['Setosa', 'Versicolor', 'Virginica']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       sepal.length  sepal.width  petal.length  petal.width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.057333      3.758000     1.199333\n",
       "std        0.828066     0.435866      1.765298     0.762238\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal.length</th>\n      <th>sepal.width</th>\n      <th>petal.length</th>\n      <th>petal.width</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.843333</td>\n      <td>3.057333</td>\n      <td>3.758000</td>\n      <td>1.199333</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.828066</td>\n      <td>0.435866</td>\n      <td>1.765298</td>\n      <td>0.762238</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.300000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.100000</td>\n      <td>2.800000</td>\n      <td>1.600000</td>\n      <td>0.300000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.800000</td>\n      <td>3.000000</td>\n      <td>4.350000</td>\n      <td>1.300000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.400000</td>\n      <td>3.300000</td>\n      <td>5.100000</td>\n      <td>1.800000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.900000</td>\n      <td>4.400000</td>\n      <td>6.900000</td>\n      <td>2.500000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "\n",
    "iris = pd.read_csv('iris.csv')\n",
    "print(iris.head())\n",
    "\n",
    "species = list(iris[\"variety\"].unique())\n",
    "print(iris['variety'].value_counts())\n",
    "\n",
    "print(species)\n",
    "iris.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": "<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>                <div id=\"6c6d7e56-334f-4828-a8f6-3ce4001af0a6\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6c6d7e56-334f-4828-a8f6-3ce4001af0a6\")) {                    Plotly.newPlot(                        \"6c6d7e56-334f-4828-a8f6-3ce4001af0a6\",                        [{\"hovertemplate\": \"variety=Setosa<br>sepal.length=%{x}<br>sepal.width=%{y}<br>petal.width=%{z}<br>petal.length=%{marker.size}<extra></extra>\", \"legendgroup\": \"Setosa\", \"marker\": {\"color\": \"#636efa\", \"opacity\": 0.7, \"size\": [1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1.5, 1.7, 1.5, 1.0, 1.7, 1.9, 1.6, 1.6, 1.5, 1.4, 1.6, 1.6, 1.5, 1.5, 1.4, 1.5, 1.2, 1.3, 1.4, 1.3, 1.5, 1.3, 1.3, 1.3, 1.6, 1.9, 1.4, 1.6, 1.4, 1.5, 1.4], \"sizemode\": \"area\", \"sizeref\": 0.01725, \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"Setosa\", \"scene\": \"scene\", \"showlegend\": true, \"type\": \"scatter3d\", \"x\": [5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 5.4, 5.1, 4.6, 5.1, 4.8, 5.0, 5.0, 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, 5.5, 4.9, 5.0, 5.5, 4.9, 4.4, 5.1, 5.0, 4.5, 4.4, 5.0, 5.1, 4.8, 5.1, 4.6, 5.3, 5.0], \"y\": [3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3.0, 3.0, 4.0, 4.4, 3.9, 3.5, 3.8, 3.8, 3.4, 3.7, 3.6, 3.3, 3.4, 3.0, 3.4, 3.5, 3.4, 3.2, 3.1, 3.4, 4.1, 4.2, 3.1, 3.2, 3.5, 3.6, 3.0, 3.4, 3.5, 2.3, 3.2, 3.5, 3.8, 3.0, 3.8, 3.2, 3.7, 3.3], \"z\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3, 0.2, 0.4, 0.2, 0.5, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 0.2, 0.2, 0.3, 0.3, 0.2, 0.6, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2]}, {\"hovertemplate\": \"variety=Versicolor<br>sepal.length=%{x}<br>sepal.width=%{y}<br>petal.width=%{z}<br>petal.length=%{marker.size}<extra></extra>\", \"legendgroup\": \"Versicolor\", \"marker\": {\"color\": \"#EF553B\", \"opacity\": 0.7, \"size\": [4.7, 4.5, 4.9, 4.0, 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4.0, 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4.0, 4.9, 4.7, 4.3, 4.4, 4.8, 5.0, 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1, 4.0, 4.4, 4.6, 4.0, 3.3, 4.2, 4.2, 4.2, 4.3, 3.0, 4.1], \"sizemode\": \"area\", \"sizeref\": 0.01725, \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"Versicolor\", \"scene\": \"scene\", \"showlegend\": true, \"type\": \"scatter3d\", \"x\": [7.0, 6.4, 6.9, 5.5, 6.5, 5.7, 6.3, 4.9, 6.6, 5.2, 5.0, 5.9, 6.0, 6.1, 5.6, 6.7, 5.6, 5.8, 6.2, 5.6, 5.9, 6.1, 6.3, 6.1, 6.4, 6.6, 6.8, 6.7, 6.0, 5.7, 5.5, 5.5, 5.8, 6.0, 5.4, 6.0, 6.7, 6.3, 5.6, 5.5, 5.5, 6.1, 5.8, 5.0, 5.6, 5.7, 5.7, 6.2, 5.1, 5.7], \"y\": [3.2, 3.2, 3.1, 2.3, 2.8, 2.8, 3.3, 2.4, 2.9, 2.7, 2.0, 3.0, 2.2, 2.9, 2.9, 3.1, 3.0, 2.7, 2.2, 2.5, 3.2, 2.8, 2.5, 2.8, 2.9, 3.0, 2.8, 3.0, 2.9, 2.6, 2.4, 2.4, 2.7, 2.7, 3.0, 3.4, 3.1, 2.3, 3.0, 2.5, 2.6, 3.0, 2.6, 2.3, 2.7, 3.0, 2.9, 2.9, 2.5, 2.8], \"z\": [1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1.0, 1.3, 1.4, 1.0, 1.5, 1.0, 1.4, 1.3, 1.4, 1.5, 1.0, 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7, 1.5, 1.0, 1.1, 1.0, 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2, 1.4, 1.2, 1.0, 1.3, 1.2, 1.3, 1.3, 1.1, 1.3]}, {\"hovertemplate\": \"variety=Virginica<br>sepal.length=%{x}<br>sepal.width=%{y}<br>petal.width=%{z}<br>petal.length=%{marker.size}<extra></extra>\", \"legendgroup\": \"Virginica\", \"marker\": {\"color\": \"#00cc96\", \"opacity\": 0.7, \"size\": [6.0, 5.1, 5.9, 5.6, 5.8, 6.6, 4.5, 6.3, 5.8, 6.1, 5.1, 5.3, 5.5, 5.0, 5.1, 5.3, 5.5, 6.7, 6.9, 5.0, 5.7, 4.9, 6.7, 4.9, 5.7, 6.0, 4.8, 4.9, 5.6, 5.8, 6.1, 6.4, 5.6, 5.1, 5.6, 6.1, 5.6, 5.5, 4.8, 5.4, 5.6, 5.1, 5.1, 5.9, 5.7, 5.2, 5.0, 5.2, 5.4, 5.1], \"sizemode\": \"area\", \"sizeref\": 0.01725, \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"Virginica\", \"scene\": \"scene\", \"showlegend\": true, \"type\": \"scatter3d\", \"x\": [6.3, 5.8, 7.1, 6.3, 6.5, 7.6, 4.9, 7.3, 6.7, 7.2, 6.5, 6.4, 6.8, 5.7, 5.8, 6.4, 6.5, 7.7, 7.7, 6.0, 6.9, 5.6, 7.7, 6.3, 6.7, 7.2, 6.2, 6.1, 6.4, 7.2, 7.4, 7.9, 6.4, 6.3, 6.1, 7.7, 6.3, 6.4, 6.0, 6.9, 6.7, 6.9, 5.8, 6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9], \"y\": [3.3, 2.7, 3.0, 2.9, 3.0, 3.0, 2.5, 2.9, 2.5, 3.6, 3.2, 2.7, 3.0, 2.5, 2.8, 3.2, 3.0, 3.8, 2.6, 2.2, 3.2, 2.8, 2.8, 2.7, 3.3, 3.2, 2.8, 3.0, 2.8, 3.0, 2.8, 3.8, 2.8, 2.8, 2.6, 3.0, 3.4, 3.1, 3.0, 3.1, 3.1, 3.1, 2.7, 3.2, 3.3, 3.0, 2.5, 3.0, 3.4, 3.0], \"z\": [2.5, 1.9, 2.1, 1.8, 2.2, 2.1, 1.7, 1.8, 1.8, 2.5, 2.0, 1.9, 2.1, 2.0, 2.4, 2.3, 1.8, 2.2, 2.3, 1.5, 2.3, 2.0, 2.0, 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6, 1.9, 2.0, 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9, 2.3, 2.5, 2.3, 1.9, 2.0, 2.3, 1.8]}],                        {\"legend\": {\"itemsizing\": \"constant\", \"title\": {\"text\": \"variety\"}, \"tracegroupgap\": 0}, \"margin\": {\"b\": 0, \"l\": 0, \"r\": 0, \"t\": 0}, \"scene\": {\"domain\": {\"x\": [0.0, 1.0], \"y\": [0.0, 1.0]}, \"xaxis\": {\"title\": {\"text\": \"sepal.length\"}}, \"yaxis\": {\"title\": {\"text\": \"sepal.width\"}}, \"zaxis\": {\"title\": {\"text\": \"petal.width\"}}}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n</body>\n</html>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "\n",
    "fig = px.scatter_3d(iris[[\"sepal.length\",\"sepal.width\",\"petal.length\",\"petal.width\",\"variety\"]],\n",
    "                    x = 'sepal.length',\n",
    "                    y = 'sepal.width',\n",
    "                    z = 'petal.width',\n",
    "                    size = 'petal.length',\n",
    "                    color = 'variety',\n",
    "                    opacity = 0.7)\n",
    "\n",
    "fig.update_layout(margin = dict(l=0, r=0, b=0, t=0))\n",
    "\n",
    "\n",
    "HTML(plotly.offline.plot(fig, filename='5d_iris_scatter.html',include_plotlyjs='cdn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": "<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>                <div id=\"2d03dc70-b357-42ab-809f-ee65cbe54ffe\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2d03dc70-b357-42ab-809f-ee65cbe54ffe\")) {                    Plotly.newPlot(                        \"2d03dc70-b357-42ab-809f-ee65cbe54ffe\",                        [{\"dimensions\": [{\"axis\": {\"matches\": true}, \"label\": \"sepal.width\", \"values\": [3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3.0, 3.0, 4.0, 4.4, 3.9, 3.5, 3.8, 3.8, 3.4, 3.7, 3.6, 3.3, 3.4, 3.0, 3.4, 3.5, 3.4, 3.2, 3.1, 3.4, 4.1, 4.2, 3.1, 3.2, 3.5, 3.6, 3.0, 3.4, 3.5, 2.3, 3.2, 3.5, 3.8, 3.0, 3.8, 3.2, 3.7, 3.3]}, {\"axis\": {\"matches\": true}, \"label\": \"sepal.length\", \"values\": [5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 5.4, 5.1, 4.6, 5.1, 4.8, 5.0, 5.0, 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, 5.5, 4.9, 5.0, 5.5, 4.9, 4.4, 5.1, 5.0, 4.5, 4.4, 5.0, 5.1, 4.8, 5.1, 4.6, 5.3, 5.0]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.width\", \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3, 0.2, 0.4, 0.2, 0.5, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 0.2, 0.2, 0.3, 0.3, 0.2, 0.6, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.length\", \"values\": [1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1.5, 1.7, 1.5, 1.0, 1.7, 1.9, 1.6, 1.6, 1.5, 1.4, 1.6, 1.6, 1.5, 1.5, 1.4, 1.5, 1.2, 1.3, 1.4, 1.3, 1.5, 1.3, 1.3, 1.3, 1.6, 1.9, 1.4, 1.6, 1.4, 1.5, 1.4]}], \"hovertemplate\": \"variety=Setosa<br>%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<extra></extra>\", \"legendgroup\": \"Setosa\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"name\": \"Setosa\", \"showlegend\": true, \"type\": \"splom\"}, {\"dimensions\": [{\"axis\": {\"matches\": true}, \"label\": \"sepal.width\", \"values\": [3.2, 3.2, 3.1, 2.3, 2.8, 2.8, 3.3, 2.4, 2.9, 2.7, 2.0, 3.0, 2.2, 2.9, 2.9, 3.1, 3.0, 2.7, 2.2, 2.5, 3.2, 2.8, 2.5, 2.8, 2.9, 3.0, 2.8, 3.0, 2.9, 2.6, 2.4, 2.4, 2.7, 2.7, 3.0, 3.4, 3.1, 2.3, 3.0, 2.5, 2.6, 3.0, 2.6, 2.3, 2.7, 3.0, 2.9, 2.9, 2.5, 2.8]}, {\"axis\": {\"matches\": true}, \"label\": \"sepal.length\", \"values\": [7.0, 6.4, 6.9, 5.5, 6.5, 5.7, 6.3, 4.9, 6.6, 5.2, 5.0, 5.9, 6.0, 6.1, 5.6, 6.7, 5.6, 5.8, 6.2, 5.6, 5.9, 6.1, 6.3, 6.1, 6.4, 6.6, 6.8, 6.7, 6.0, 5.7, 5.5, 5.5, 5.8, 6.0, 5.4, 6.0, 6.7, 6.3, 5.6, 5.5, 5.5, 6.1, 5.8, 5.0, 5.6, 5.7, 5.7, 6.2, 5.1, 5.7]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.width\", \"values\": [1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1.0, 1.3, 1.4, 1.0, 1.5, 1.0, 1.4, 1.3, 1.4, 1.5, 1.0, 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7, 1.5, 1.0, 1.1, 1.0, 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2, 1.4, 1.2, 1.0, 1.3, 1.2, 1.3, 1.3, 1.1, 1.3]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.length\", \"values\": [4.7, 4.5, 4.9, 4.0, 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4.0, 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4.0, 4.9, 4.7, 4.3, 4.4, 4.8, 5.0, 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1, 4.0, 4.4, 4.6, 4.0, 3.3, 4.2, 4.2, 4.2, 4.3, 3.0, 4.1]}], \"hovertemplate\": \"variety=Versicolor<br>%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<extra></extra>\", \"legendgroup\": \"Versicolor\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"name\": \"Versicolor\", \"showlegend\": true, \"type\": \"splom\"}, {\"dimensions\": [{\"axis\": {\"matches\": true}, \"label\": \"sepal.width\", \"values\": [3.3, 2.7, 3.0, 2.9, 3.0, 3.0, 2.5, 2.9, 2.5, 3.6, 3.2, 2.7, 3.0, 2.5, 2.8, 3.2, 3.0, 3.8, 2.6, 2.2, 3.2, 2.8, 2.8, 2.7, 3.3, 3.2, 2.8, 3.0, 2.8, 3.0, 2.8, 3.8, 2.8, 2.8, 2.6, 3.0, 3.4, 3.1, 3.0, 3.1, 3.1, 3.1, 2.7, 3.2, 3.3, 3.0, 2.5, 3.0, 3.4, 3.0]}, {\"axis\": {\"matches\": true}, \"label\": \"sepal.length\", \"values\": [6.3, 5.8, 7.1, 6.3, 6.5, 7.6, 4.9, 7.3, 6.7, 7.2, 6.5, 6.4, 6.8, 5.7, 5.8, 6.4, 6.5, 7.7, 7.7, 6.0, 6.9, 5.6, 7.7, 6.3, 6.7, 7.2, 6.2, 6.1, 6.4, 7.2, 7.4, 7.9, 6.4, 6.3, 6.1, 7.7, 6.3, 6.4, 6.0, 6.9, 6.7, 6.9, 5.8, 6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.width\", \"values\": [2.5, 1.9, 2.1, 1.8, 2.2, 2.1, 1.7, 1.8, 1.8, 2.5, 2.0, 1.9, 2.1, 2.0, 2.4, 2.3, 1.8, 2.2, 2.3, 1.5, 2.3, 2.0, 2.0, 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6, 1.9, 2.0, 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9, 2.3, 2.5, 2.3, 1.9, 2.0, 2.3, 1.8]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.length\", \"values\": [6.0, 5.1, 5.9, 5.6, 5.8, 6.6, 4.5, 6.3, 5.8, 6.1, 5.1, 5.3, 5.5, 5.0, 5.1, 5.3, 5.5, 6.7, 6.9, 5.0, 5.7, 4.9, 6.7, 4.9, 5.7, 6.0, 4.8, 4.9, 5.6, 5.8, 6.1, 6.4, 5.6, 5.1, 5.6, 6.1, 5.6, 5.5, 4.8, 5.4, 5.6, 5.1, 5.1, 5.9, 5.7, 5.2, 5.0, 5.2, 5.4, 5.1]}], \"hovertemplate\": \"variety=Virginica<br>%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<extra></extra>\", \"legendgroup\": \"Virginica\", \"marker\": {\"color\": \"#00cc96\", \"symbol\": \"circle\"}, \"name\": \"Virginica\", \"showlegend\": true, \"type\": \"splom\"}],                        {\"dragmode\": \"select\", \"legend\": {\"title\": {\"text\": \"variety\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n</body>\n</html>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "fig = px.scatter_matrix(iris, dimensions=[\"sepal.width\", \"sepal.length\", \"petal.width\", \"petal.length\"],color=\"variety\")\n",
    "HTML(plotly.offline.plot(fig, filename='5d_scatter_matrix.html',include_plotlyjs='cdn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input columns all:  torch.Size([150, 4]) torch.float32\nInput columns:  torch.Size([150, 2]) torch.float32\nOutput columns:  torch.Size([150]) torch.int8\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['petal.length', 'petal.width']\n",
    "\n",
    "input_columns_all = torch.from_numpy(iris[list(iris.columns)[:-1]].to_numpy()).type(torch.float32)\n",
    "input_columns = torch.from_numpy(iris[selected_features].to_numpy()).type(torch.float32)\n",
    "output_columns = torch.tensor(iris['variety'].astype('category').cat.codes)\n",
    "\n",
    "print(\"Input columns all: \", input_columns_all.shape, input_columns_all.dtype)\n",
    "print(\"Input columns: \", input_columns.shape, input_columns.dtype)\n",
    "print(\"Output columns: \", output_columns.shape, output_columns.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.utils.data.TensorDataset(input_columns, output_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.1\n",
    "rows = list(input_columns.shape)[0]\n",
    "test_split = int(rows*split)\n",
    "val_split = int(rows*split*2)\n",
    "train_split = rows - val_split - test_split\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(data, [train_split, val_split, test_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, 16, shuffle = True) #batch size = 16\n",
    "val_loader = torch.utils.data.DataLoader(val_set) #batch size = 1\n",
    "test_loader = torch.utils.data.DataLoader(test_set) #batch size = 1"
   ]
  },
  {
   "source": [
    "Cross entropy loss is the predicted probability compared to how far that is from the actual value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dimension, output_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, targets.long())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, targets.long())\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        # Calculate the number of correct predictions over the number of predictions\n",
    "        accuracy = torch.tensor(torch.sum(pred==targets).item()/len(pred))\n",
    "        return [loss.detach(), accuracy.detach()] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, epochs, learning_rate, criterion_function = torch.nn.functional.cross_entropy, optimizer_function = torch.optim.Adam):\n",
    "    history = {\"loss\" : [], \"accuracy\" : []}\n",
    "    optimizer = optimizer_function(model.parameters(), learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch \", epoch)\n",
    "        #Train\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        #Validate\n",
    "        for batch in val_loader:\n",
    "            loss, accuracy = evaluate(model, val_loader)\n",
    "        print(\"loss: \", loss.item(), \"accuracy: \", accuracy.item(), \"\\n\")\n",
    "        history[\"loss\"].append(loss.item())\n",
    "        history[\"accuracy\"].append(accuracy.item())\n",
    "         \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    outputs = [model.validation_step(batch) for batch in loader]\n",
    "    outputs = torch.tensor(outputs).T\n",
    "    loss, accuracy = torch.mean(outputs, dim=1)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch  0\n",
      "loss:  2.030327320098877 accuracy:  0.4000000059604645 \n",
      "\n",
      "Epoch  1\n",
      "loss:  1.7610653638839722 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  2\n",
      "loss:  1.5293662548065186 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  3\n",
      "loss:  1.3294787406921387 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  4\n",
      "loss:  1.1859362125396729 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  5\n",
      "loss:  1.1106562614440918 accuracy:  0.2666666805744171 \n",
      "\n",
      "Epoch  6\n",
      "loss:  1.0785987377166748 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  7\n",
      "loss:  1.0622694492340088 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  8\n",
      "loss:  1.0444947481155396 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  9\n",
      "loss:  1.0267270803451538 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  10\n",
      "loss:  1.0071864128112793 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  11\n",
      "loss:  0.986542820930481 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  12\n",
      "loss:  0.9669081568717957 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  13\n",
      "loss:  0.9469481110572815 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  14\n",
      "loss:  0.9274236559867859 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  15\n",
      "loss:  0.9105685353279114 accuracy:  0.3333333432674408 \n",
      "\n",
      "Epoch  16\n",
      "loss:  0.8952395915985107 accuracy:  0.5666666626930237 \n",
      "\n",
      "Epoch  17\n",
      "loss:  0.8774256110191345 accuracy:  0.6333333253860474 \n",
      "\n",
      "Epoch  18\n",
      "loss:  0.8615209460258484 accuracy:  0.6333333253860474 \n",
      "\n",
      "Epoch  19\n",
      "loss:  0.8450669646263123 accuracy:  0.6666666865348816 \n",
      "\n",
      "Epoch  20\n",
      "loss:  0.8303794860839844 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  21\n",
      "loss:  0.8138881325721741 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  22\n",
      "loss:  0.7995455861091614 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  23\n",
      "loss:  0.7860969305038452 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  24\n",
      "loss:  0.7716020345687866 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  25\n",
      "loss:  0.759977400302887 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  26\n",
      "loss:  0.7482830286026001 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  27\n",
      "loss:  0.7353783249855042 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  28\n",
      "loss:  0.7253599166870117 accuracy:  0.7333333492279053 \n",
      "\n",
      "Epoch  29\n",
      "loss:  0.7136319279670715 accuracy:  0.7333333492279053 \n",
      "\n",
      "Epoch  30\n",
      "loss:  0.7012836933135986 accuracy:  0.7333333492279053 \n",
      "\n",
      "Epoch  31\n",
      "loss:  0.6901937127113342 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  32\n",
      "loss:  0.6796579360961914 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  33\n",
      "loss:  0.6687695980072021 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  34\n",
      "loss:  0.6603279113769531 accuracy:  0.7333333492279053 \n",
      "\n",
      "Epoch  35\n",
      "loss:  0.6509579420089722 accuracy:  0.7333333492279053 \n",
      "\n",
      "Epoch  36\n",
      "loss:  0.641491174697876 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  37\n",
      "loss:  0.6332687139511108 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  38\n",
      "loss:  0.62464839220047 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  39\n",
      "loss:  0.616381824016571 accuracy:  0.7666666507720947 \n",
      "\n",
      "Epoch  40\n",
      "loss:  0.6088261008262634 accuracy:  0.7666666507720947 \n",
      "\n",
      "Epoch  41\n",
      "loss:  0.6014447808265686 accuracy:  0.7666666507720947 \n",
      "\n",
      "Epoch  42\n",
      "loss:  0.5936964154243469 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  43\n",
      "loss:  0.5861839056015015 accuracy:  0.7666666507720947 \n",
      "\n",
      "Epoch  44\n",
      "loss:  0.5798320770263672 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  45\n",
      "loss:  0.5726810693740845 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  46\n",
      "loss:  0.5657742023468018 accuracy:  0.8333333134651184 \n",
      "\n",
      "Epoch  47\n",
      "loss:  0.5597934722900391 accuracy:  0.7666666507720947 \n",
      "\n",
      "Epoch  48\n",
      "loss:  0.5543763637542725 accuracy:  0.7666666507720947 \n",
      "\n",
      "Epoch  49\n",
      "loss:  0.5473031401634216 accuracy:  0.8333333134651184 \n",
      "\n",
      "Epoch  50\n",
      "loss:  0.5413721799850464 accuracy:  0.8333333134651184 \n",
      "\n",
      "Epoch  51\n",
      "loss:  0.5362970232963562 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  52\n",
      "loss:  0.5312835574150085 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  53\n",
      "loss:  0.5257012248039246 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  54\n",
      "loss:  0.520134687423706 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  55\n",
      "loss:  0.5153153538703918 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  56\n",
      "loss:  0.5105029940605164 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  57\n",
      "loss:  0.5055833458900452 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  58\n",
      "loss:  0.5007675886154175 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  59\n",
      "loss:  0.4958680272102356 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  60\n",
      "loss:  0.49165984988212585 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  61\n",
      "loss:  0.4877915680408478 accuracy:  0.8333333134651184 \n",
      "\n",
      "Epoch  62\n",
      "loss:  0.4832989573478699 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  63\n",
      "loss:  0.4788911044597626 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  64\n",
      "loss:  0.4748191237449646 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  65\n",
      "loss:  0.47068166732788086 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  66\n",
      "loss:  0.4665779769420624 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  67\n",
      "loss:  0.4624937176704407 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  68\n",
      "loss:  0.45877915620803833 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  69\n",
      "loss:  0.45490318536758423 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  70\n",
      "loss:  0.4512789845466614 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  71\n",
      "loss:  0.44825008511543274 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  72\n",
      "loss:  0.44436562061309814 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  73\n",
      "loss:  0.44094088673591614 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  74\n",
      "loss:  0.4380791485309601 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  75\n",
      "loss:  0.43458622694015503 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  76\n",
      "loss:  0.43146952986717224 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  77\n",
      "loss:  0.428569495677948 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  78\n",
      "loss:  0.4251595139503479 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  79\n",
      "loss:  0.42176875472068787 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  80\n",
      "loss:  0.4197739362716675 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  81\n",
      "loss:  0.4158804714679718 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  82\n",
      "loss:  0.4128824472427368 accuracy:  1.0 \n",
      "\n",
      "Epoch  83\n",
      "loss:  0.40980762243270874 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  84\n",
      "loss:  0.40764468908309937 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  85\n",
      "loss:  0.40555238723754883 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  86\n",
      "loss:  0.4023183584213257 accuracy:  1.0 \n",
      "\n",
      "Epoch  87\n",
      "loss:  0.3990212678909302 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  88\n",
      "loss:  0.39644578099250793 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  89\n",
      "loss:  0.39386215806007385 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  90\n",
      "loss:  0.3915807008743286 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  91\n",
      "loss:  0.39058759808540344 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  92\n",
      "loss:  0.3874635398387909 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  93\n",
      "loss:  0.3839736878871918 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  94\n",
      "loss:  0.38151437044143677 accuracy:  1.0 \n",
      "\n",
      "Epoch  95\n",
      "loss:  0.37910333275794983 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  96\n",
      "loss:  0.37949466705322266 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  97\n",
      "loss:  0.3765046298503876 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  98\n",
      "loss:  0.3726910352706909 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  99\n",
      "loss:  0.3702943027019501 accuracy:  1.0 \n",
      "\n",
      "Epoch  100\n",
      "loss:  0.3683549761772156 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  101\n",
      "loss:  0.3658682405948639 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  102\n",
      "loss:  0.3638477623462677 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  103\n",
      "loss:  0.3616662323474884 accuracy:  1.0 \n",
      "\n",
      "Epoch  104\n",
      "loss:  0.3602526783943176 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  105\n",
      "loss:  0.35755810141563416 accuracy:  1.0 \n",
      "\n",
      "Epoch  106\n",
      "loss:  0.3558539152145386 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  107\n",
      "loss:  0.35383477807044983 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  108\n",
      "loss:  0.3515686094760895 accuracy:  1.0 \n",
      "\n",
      "Epoch  109\n",
      "loss:  0.34949249029159546 accuracy:  1.0 \n",
      "\n",
      "Epoch  110\n",
      "loss:  0.3475331664085388 accuracy:  1.0 \n",
      "\n",
      "Epoch  111\n",
      "loss:  0.34566977620124817 accuracy:  1.0 \n",
      "\n",
      "Epoch  112\n",
      "loss:  0.3445895314216614 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  113\n",
      "loss:  0.3420582711696625 accuracy:  1.0 \n",
      "\n",
      "Epoch  114\n",
      "loss:  0.33986896276474 accuracy:  1.0 \n",
      "\n",
      "Epoch  115\n",
      "loss:  0.3386296033859253 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  116\n",
      "loss:  0.33662739396095276 accuracy:  1.0 \n",
      "\n",
      "Epoch  117\n",
      "loss:  0.33485323190689087 accuracy:  1.0 \n",
      "\n",
      "Epoch  118\n",
      "loss:  0.33292123675346375 accuracy:  1.0 \n",
      "\n",
      "Epoch  119\n",
      "loss:  0.33111482858657837 accuracy:  1.0 \n",
      "\n",
      "Epoch  120\n",
      "loss:  0.3294927179813385 accuracy:  1.0 \n",
      "\n",
      "Epoch  121\n",
      "loss:  0.32953083515167236 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  122\n",
      "loss:  0.326590359210968 accuracy:  1.0 \n",
      "\n",
      "Epoch  123\n",
      "loss:  0.32435911893844604 accuracy:  1.0 \n",
      "\n",
      "Epoch  124\n",
      "loss:  0.3225148916244507 accuracy:  1.0 \n",
      "\n",
      "Epoch  125\n",
      "loss:  0.32147321105003357 accuracy:  1.0 \n",
      "\n",
      "Epoch  126\n",
      "loss:  0.3203137516975403 accuracy:  1.0 \n",
      "\n",
      "Epoch  127\n",
      "loss:  0.31810933351516724 accuracy:  1.0 \n",
      "\n",
      "Epoch  128\n",
      "loss:  0.31625765562057495 accuracy:  1.0 \n",
      "\n",
      "Epoch  129\n",
      "loss:  0.31453564763069153 accuracy:  1.0 \n",
      "\n",
      "Epoch  130\n",
      "loss:  0.31302356719970703 accuracy:  1.0 \n",
      "\n",
      "Epoch  131\n",
      "loss:  0.3124646246433258 accuracy:  1.0 \n",
      "\n",
      "Epoch  132\n",
      "loss:  0.3104029893875122 accuracy:  1.0 \n",
      "\n",
      "Epoch  133\n",
      "loss:  0.3086971342563629 accuracy:  1.0 \n",
      "\n",
      "Epoch  134\n",
      "loss:  0.3073066771030426 accuracy:  1.0 \n",
      "\n",
      "Epoch  135\n",
      "loss:  0.30531853437423706 accuracy:  1.0 \n",
      "\n",
      "Epoch  136\n",
      "loss:  0.30375462770462036 accuracy:  1.0 \n",
      "\n",
      "Epoch  137\n",
      "loss:  0.30221468210220337 accuracy:  1.0 \n",
      "\n",
      "Epoch  138\n",
      "loss:  0.3021158277988434 accuracy:  1.0 \n",
      "\n",
      "Epoch  139\n",
      "loss:  0.30013710260391235 accuracy:  1.0 \n",
      "\n",
      "Epoch  140\n",
      "loss:  0.29813826084136963 accuracy:  1.0 \n",
      "\n",
      "Epoch  141\n",
      "loss:  0.2970631420612335 accuracy:  1.0 \n",
      "\n",
      "Epoch  142\n",
      "loss:  0.29788148403167725 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  143\n",
      "loss:  0.29556533694267273 accuracy:  1.0 \n",
      "\n",
      "Epoch  144\n",
      "loss:  0.29241979122161865 accuracy:  1.0 \n",
      "\n",
      "Epoch  145\n",
      "loss:  0.2910461127758026 accuracy:  1.0 \n",
      "\n",
      "Epoch  146\n",
      "loss:  0.29045993089675903 accuracy:  1.0 \n",
      "\n",
      "Epoch  147\n",
      "loss:  0.291023313999176 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  148\n",
      "loss:  0.28737062215805054 accuracy:  1.0 \n",
      "\n",
      "Epoch  149\n",
      "loss:  0.2856736183166504 accuracy:  1.0 \n",
      "\n",
      "Epoch  150\n",
      "loss:  0.2856941819190979 accuracy:  1.0 \n",
      "\n",
      "Epoch  151\n",
      "loss:  0.28290969133377075 accuracy:  1.0 \n",
      "\n",
      "Epoch  152\n",
      "loss:  0.2822638154029846 accuracy:  1.0 \n",
      "\n",
      "Epoch  153\n",
      "loss:  0.2805647552013397 accuracy:  1.0 \n",
      "\n",
      "Epoch  154\n",
      "loss:  0.2790611982345581 accuracy:  1.0 \n",
      "\n",
      "Epoch  155\n",
      "loss:  0.27793779969215393 accuracy:  1.0 \n",
      "\n",
      "Epoch  156\n",
      "loss:  0.27839022874832153 accuracy:  1.0 \n",
      "\n",
      "Epoch  157\n",
      "loss:  0.2755943536758423 accuracy:  1.0 \n",
      "\n",
      "Epoch  158\n",
      "loss:  0.27464351058006287 accuracy:  1.0 \n",
      "\n",
      "Epoch  159\n",
      "loss:  0.2731136381626129 accuracy:  1.0 \n",
      "\n",
      "Epoch  160\n",
      "loss:  0.27201148867607117 accuracy:  1.0 \n",
      "\n",
      "Epoch  161\n",
      "loss:  0.2738944888114929 accuracy:  1.0 \n",
      "\n",
      "Epoch  162\n",
      "loss:  0.27230122685432434 accuracy:  1.0 \n",
      "\n",
      "Epoch  163\n",
      "loss:  0.26837435364723206 accuracy:  1.0 \n",
      "\n",
      "Epoch  164\n",
      "loss:  0.2669973373413086 accuracy:  1.0 \n",
      "\n",
      "Epoch  165\n",
      "loss:  0.2657943367958069 accuracy:  1.0 \n",
      "\n",
      "Epoch  166\n",
      "loss:  0.26533856987953186 accuracy:  1.0 \n",
      "\n",
      "Epoch  167\n",
      "loss:  0.2663209140300751 accuracy:  1.0 \n",
      "\n",
      "Epoch  168\n",
      "loss:  0.2638227343559265 accuracy:  1.0 \n",
      "\n",
      "Epoch  169\n",
      "loss:  0.26239725947380066 accuracy:  1.0 \n",
      "\n",
      "Epoch  170\n",
      "loss:  0.2609494626522064 accuracy:  1.0 \n",
      "\n",
      "Epoch  171\n",
      "loss:  0.2602802515029907 accuracy:  1.0 \n",
      "\n",
      "Epoch  172\n",
      "loss:  0.25848057866096497 accuracy:  1.0 \n",
      "\n",
      "Epoch  173\n",
      "loss:  0.2574470341205597 accuracy:  1.0 \n",
      "\n",
      "Epoch  174\n",
      "loss:  0.25746408104896545 accuracy:  1.0 \n",
      "\n",
      "Epoch  175\n",
      "loss:  0.2555072605609894 accuracy:  1.0 \n",
      "\n",
      "Epoch  176\n",
      "loss:  0.25478020310401917 accuracy:  1.0 \n",
      "\n",
      "Epoch  177\n",
      "loss:  0.25319430232048035 accuracy:  1.0 \n",
      "\n",
      "Epoch  178\n",
      "loss:  0.25211548805236816 accuracy:  1.0 \n",
      "\n",
      "Epoch  179\n",
      "loss:  0.2525476813316345 accuracy:  1.0 \n",
      "\n",
      "Epoch  180\n",
      "loss:  0.249985009431839 accuracy:  1.0 \n",
      "\n",
      "Epoch  181\n",
      "loss:  0.24985434114933014 accuracy:  1.0 \n",
      "\n",
      "Epoch  182\n",
      "loss:  0.24756397306919098 accuracy:  1.0 \n",
      "\n",
      "Epoch  183\n",
      "loss:  0.24738852679729462 accuracy:  1.0 \n",
      "\n",
      "Epoch  184\n",
      "loss:  0.24578703939914703 accuracy:  1.0 \n",
      "\n",
      "Epoch  185\n",
      "loss:  0.24526968598365784 accuracy:  1.0 \n",
      "\n",
      "Epoch  186\n",
      "loss:  0.24348901212215424 accuracy:  1.0 \n",
      "\n",
      "Epoch  187\n",
      "loss:  0.24616718292236328 accuracy:  1.0 \n",
      "\n",
      "Epoch  188\n",
      "loss:  0.24292710423469543 accuracy:  1.0 \n",
      "\n",
      "Epoch  189\n",
      "loss:  0.2409108728170395 accuracy:  1.0 \n",
      "\n",
      "Epoch  190\n",
      "loss:  0.24098652601242065 accuracy:  1.0 \n",
      "\n",
      "Epoch  191\n",
      "loss:  0.2410566359758377 accuracy:  1.0 \n",
      "\n",
      "Epoch  192\n",
      "loss:  0.2385939657688141 accuracy:  1.0 \n",
      "\n",
      "Epoch  193\n",
      "loss:  0.23953460156917572 accuracy:  1.0 \n",
      "\n",
      "Epoch  194\n",
      "loss:  0.23652121424674988 accuracy:  1.0 \n",
      "\n",
      "Epoch  195\n",
      "loss:  0.23502451181411743 accuracy:  1.0 \n",
      "\n",
      "Epoch  196\n",
      "loss:  0.23439893126487732 accuracy:  1.0 \n",
      "\n",
      "Epoch  197\n",
      "loss:  0.23476696014404297 accuracy:  1.0 \n",
      "\n",
      "Epoch  198\n",
      "loss:  0.23403987288475037 accuracy:  1.0 \n",
      "\n",
      "Epoch  199\n",
      "loss:  0.23330923914909363 accuracy:  1.0 \n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': [2.030327320098877,\n",
       "  1.7610653638839722,\n",
       "  1.5293662548065186,\n",
       "  1.3294787406921387,\n",
       "  1.1859362125396729,\n",
       "  1.1106562614440918,\n",
       "  1.0785987377166748,\n",
       "  1.0622694492340088,\n",
       "  1.0444947481155396,\n",
       "  1.0267270803451538,\n",
       "  1.0071864128112793,\n",
       "  0.986542820930481,\n",
       "  0.9669081568717957,\n",
       "  0.9469481110572815,\n",
       "  0.9274236559867859,\n",
       "  0.9105685353279114,\n",
       "  0.8952395915985107,\n",
       "  0.8774256110191345,\n",
       "  0.8615209460258484,\n",
       "  0.8450669646263123,\n",
       "  0.8303794860839844,\n",
       "  0.8138881325721741,\n",
       "  0.7995455861091614,\n",
       "  0.7860969305038452,\n",
       "  0.7716020345687866,\n",
       "  0.759977400302887,\n",
       "  0.7482830286026001,\n",
       "  0.7353783249855042,\n",
       "  0.7253599166870117,\n",
       "  0.7136319279670715,\n",
       "  0.7012836933135986,\n",
       "  0.6901937127113342,\n",
       "  0.6796579360961914,\n",
       "  0.6687695980072021,\n",
       "  0.6603279113769531,\n",
       "  0.6509579420089722,\n",
       "  0.641491174697876,\n",
       "  0.6332687139511108,\n",
       "  0.62464839220047,\n",
       "  0.616381824016571,\n",
       "  0.6088261008262634,\n",
       "  0.6014447808265686,\n",
       "  0.5936964154243469,\n",
       "  0.5861839056015015,\n",
       "  0.5798320770263672,\n",
       "  0.5726810693740845,\n",
       "  0.5657742023468018,\n",
       "  0.5597934722900391,\n",
       "  0.5543763637542725,\n",
       "  0.5473031401634216,\n",
       "  0.5413721799850464,\n",
       "  0.5362970232963562,\n",
       "  0.5312835574150085,\n",
       "  0.5257012248039246,\n",
       "  0.520134687423706,\n",
       "  0.5153153538703918,\n",
       "  0.5105029940605164,\n",
       "  0.5055833458900452,\n",
       "  0.5007675886154175,\n",
       "  0.4958680272102356,\n",
       "  0.49165984988212585,\n",
       "  0.4877915680408478,\n",
       "  0.4832989573478699,\n",
       "  0.4788911044597626,\n",
       "  0.4748191237449646,\n",
       "  0.47068166732788086,\n",
       "  0.4665779769420624,\n",
       "  0.4624937176704407,\n",
       "  0.45877915620803833,\n",
       "  0.45490318536758423,\n",
       "  0.4512789845466614,\n",
       "  0.44825008511543274,\n",
       "  0.44436562061309814,\n",
       "  0.44094088673591614,\n",
       "  0.4380791485309601,\n",
       "  0.43458622694015503,\n",
       "  0.43146952986717224,\n",
       "  0.428569495677948,\n",
       "  0.4251595139503479,\n",
       "  0.42176875472068787,\n",
       "  0.4197739362716675,\n",
       "  0.4158804714679718,\n",
       "  0.4128824472427368,\n",
       "  0.40980762243270874,\n",
       "  0.40764468908309937,\n",
       "  0.40555238723754883,\n",
       "  0.4023183584213257,\n",
       "  0.3990212678909302,\n",
       "  0.39644578099250793,\n",
       "  0.39386215806007385,\n",
       "  0.3915807008743286,\n",
       "  0.39058759808540344,\n",
       "  0.3874635398387909,\n",
       "  0.3839736878871918,\n",
       "  0.38151437044143677,\n",
       "  0.37910333275794983,\n",
       "  0.37949466705322266,\n",
       "  0.3765046298503876,\n",
       "  0.3726910352706909,\n",
       "  0.3702943027019501,\n",
       "  0.3683549761772156,\n",
       "  0.3658682405948639,\n",
       "  0.3638477623462677,\n",
       "  0.3616662323474884,\n",
       "  0.3602526783943176,\n",
       "  0.35755810141563416,\n",
       "  0.3558539152145386,\n",
       "  0.35383477807044983,\n",
       "  0.3515686094760895,\n",
       "  0.34949249029159546,\n",
       "  0.3475331664085388,\n",
       "  0.34566977620124817,\n",
       "  0.3445895314216614,\n",
       "  0.3420582711696625,\n",
       "  0.33986896276474,\n",
       "  0.3386296033859253,\n",
       "  0.33662739396095276,\n",
       "  0.33485323190689087,\n",
       "  0.33292123675346375,\n",
       "  0.33111482858657837,\n",
       "  0.3294927179813385,\n",
       "  0.32953083515167236,\n",
       "  0.326590359210968,\n",
       "  0.32435911893844604,\n",
       "  0.3225148916244507,\n",
       "  0.32147321105003357,\n",
       "  0.3203137516975403,\n",
       "  0.31810933351516724,\n",
       "  0.31625765562057495,\n",
       "  0.31453564763069153,\n",
       "  0.31302356719970703,\n",
       "  0.3124646246433258,\n",
       "  0.3104029893875122,\n",
       "  0.3086971342563629,\n",
       "  0.3073066771030426,\n",
       "  0.30531853437423706,\n",
       "  0.30375462770462036,\n",
       "  0.30221468210220337,\n",
       "  0.3021158277988434,\n",
       "  0.30013710260391235,\n",
       "  0.29813826084136963,\n",
       "  0.2970631420612335,\n",
       "  0.29788148403167725,\n",
       "  0.29556533694267273,\n",
       "  0.29241979122161865,\n",
       "  0.2910461127758026,\n",
       "  0.29045993089675903,\n",
       "  0.291023313999176,\n",
       "  0.28737062215805054,\n",
       "  0.2856736183166504,\n",
       "  0.2856941819190979,\n",
       "  0.28290969133377075,\n",
       "  0.2822638154029846,\n",
       "  0.2805647552013397,\n",
       "  0.2790611982345581,\n",
       "  0.27793779969215393,\n",
       "  0.27839022874832153,\n",
       "  0.2755943536758423,\n",
       "  0.27464351058006287,\n",
       "  0.2731136381626129,\n",
       "  0.27201148867607117,\n",
       "  0.2738944888114929,\n",
       "  0.27230122685432434,\n",
       "  0.26837435364723206,\n",
       "  0.2669973373413086,\n",
       "  0.2657943367958069,\n",
       "  0.26533856987953186,\n",
       "  0.2663209140300751,\n",
       "  0.2638227343559265,\n",
       "  0.26239725947380066,\n",
       "  0.2609494626522064,\n",
       "  0.2602802515029907,\n",
       "  0.25848057866096497,\n",
       "  0.2574470341205597,\n",
       "  0.25746408104896545,\n",
       "  0.2555072605609894,\n",
       "  0.25478020310401917,\n",
       "  0.25319430232048035,\n",
       "  0.25211548805236816,\n",
       "  0.2525476813316345,\n",
       "  0.249985009431839,\n",
       "  0.24985434114933014,\n",
       "  0.24756397306919098,\n",
       "  0.24738852679729462,\n",
       "  0.24578703939914703,\n",
       "  0.24526968598365784,\n",
       "  0.24348901212215424,\n",
       "  0.24616718292236328,\n",
       "  0.24292710423469543,\n",
       "  0.2409108728170395,\n",
       "  0.24098652601242065,\n",
       "  0.2410566359758377,\n",
       "  0.2385939657688141,\n",
       "  0.23953460156917572,\n",
       "  0.23652121424674988,\n",
       "  0.23502451181411743,\n",
       "  0.23439893126487732,\n",
       "  0.23476696014404297,\n",
       "  0.23403987288475037,\n",
       "  0.23330923914909363],\n",
       " 'accuracy': [0.4000000059604645,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.2666666805744171,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.3333333432674408,\n",
       "  0.5666666626930237,\n",
       "  0.6333333253860474,\n",
       "  0.6333333253860474,\n",
       "  0.6666666865348816,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.7666666507720947,\n",
       "  0.7666666507720947,\n",
       "  0.7666666507720947,\n",
       "  0.800000011920929,\n",
       "  0.7666666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.800000011920929,\n",
       "  0.8333333134651184,\n",
       "  0.7666666507720947,\n",
       "  0.7666666507720947,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8999999761581421,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.9333333373069763,\n",
       "  0.8666666746139526,\n",
       "  0.9333333373069763,\n",
       "  0.8999999761581421,\n",
       "  0.9333333373069763,\n",
       "  0.8333333134651184,\n",
       "  0.8666666746139526,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LogisticRegression(len(selected_features), len(species))\n",
    "fit(model, train_loader, val_loader, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch  0\n",
      "loss:  0.9214321374893188 accuracy:  0.20000000298023224 \n",
      "\n",
      "Epoch  1\n",
      "loss:  0.8374913334846497 accuracy:  0.30000001192092896 \n",
      "\n",
      "Epoch  2\n",
      "loss:  0.7913421392440796 accuracy:  0.5 \n",
      "\n",
      "Epoch  3\n",
      "loss:  0.7314777374267578 accuracy:  0.5666666626930237 \n",
      "\n",
      "Epoch  4\n",
      "loss:  0.7106940746307373 accuracy:  0.5666666626930237 \n",
      "\n",
      "Epoch  5\n",
      "loss:  0.6411460041999817 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  6\n",
      "loss:  0.6133079528808594 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  7\n",
      "loss:  0.6007789969444275 accuracy:  0.6333333253860474 \n",
      "\n",
      "Epoch  8\n",
      "loss:  0.5849729776382446 accuracy:  0.6000000238418579 \n",
      "\n",
      "Epoch  9\n",
      "loss:  0.5588006973266602 accuracy:  0.6666666865348816 \n",
      "\n",
      "Epoch  10\n",
      "loss:  0.514543890953064 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  11\n",
      "loss:  0.49819281697273254 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  12\n",
      "loss:  0.5612049698829651 accuracy:  0.5666666626930237 \n",
      "\n",
      "Epoch  13\n",
      "loss:  0.4893837571144104 accuracy:  0.7666666507720947 \n",
      "\n",
      "Epoch  14\n",
      "loss:  0.4779756963253021 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  15\n",
      "loss:  0.46778708696365356 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  16\n",
      "loss:  0.4718855023384094 accuracy:  0.7333333492279053 \n",
      "\n",
      "Epoch  17\n",
      "loss:  0.4483048617839813 accuracy:  0.8333333134651184 \n",
      "\n",
      "Epoch  18\n",
      "loss:  0.439182847738266 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  19\n",
      "loss:  0.4527899920940399 accuracy:  0.7333333492279053 \n",
      "\n",
      "Epoch  20\n",
      "loss:  0.433691143989563 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  21\n",
      "loss:  0.4162185490131378 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  22\n",
      "loss:  0.4128153622150421 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  23\n",
      "loss:  0.43063607811927795 accuracy:  0.7333333492279053 \n",
      "\n",
      "Epoch  24\n",
      "loss:  0.37359973788261414 accuracy:  1.0 \n",
      "\n",
      "Epoch  25\n",
      "loss:  0.37959957122802734 accuracy:  1.0 \n",
      "\n",
      "Epoch  26\n",
      "loss:  0.44972339272499084 accuracy:  0.6666666865348816 \n",
      "\n",
      "Epoch  27\n",
      "loss:  0.38677018880844116 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  28\n",
      "loss:  0.35484185814857483 accuracy:  1.0 \n",
      "\n",
      "Epoch  29\n",
      "loss:  0.3950134217739105 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  30\n",
      "loss:  0.38690510392189026 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  31\n",
      "loss:  0.3768267333507538 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  32\n",
      "loss:  0.37460121512413025 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  33\n",
      "loss:  0.35453474521636963 accuracy:  1.0 \n",
      "\n",
      "Epoch  34\n",
      "loss:  0.3799147307872772 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  35\n",
      "loss:  0.34729066491127014 accuracy:  1.0 \n",
      "\n",
      "Epoch  36\n",
      "loss:  0.3360620439052582 accuracy:  1.0 \n",
      "\n",
      "Epoch  37\n",
      "loss:  0.34364011883735657 accuracy:  1.0 \n",
      "\n",
      "Epoch  38\n",
      "loss:  0.3378128409385681 accuracy:  1.0 \n",
      "\n",
      "Epoch  39\n",
      "loss:  0.34443894028663635 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  40\n",
      "loss:  0.3192448019981384 accuracy:  1.0 \n",
      "\n",
      "Epoch  41\n",
      "loss:  0.3437041640281677 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  42\n",
      "loss:  0.34749093651771545 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  43\n",
      "loss:  0.3067064583301544 accuracy:  1.0 \n",
      "\n",
      "Epoch  44\n",
      "loss:  0.297076016664505 accuracy:  1.0 \n",
      "\n",
      "Epoch  45\n",
      "loss:  0.3299659192562103 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  46\n",
      "loss:  0.3246581256389618 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  47\n",
      "loss:  0.3027423620223999 accuracy:  1.0 \n",
      "\n",
      "Epoch  48\n",
      "loss:  0.3000412881374359 accuracy:  1.0 \n",
      "\n",
      "Epoch  49\n",
      "loss:  0.3049972355365753 accuracy:  1.0 \n",
      "\n",
      "Epoch  50\n",
      "loss:  0.3073378801345825 accuracy:  1.0 \n",
      "\n",
      "Epoch  51\n",
      "loss:  0.30280572175979614 accuracy:  1.0 \n",
      "\n",
      "Epoch  52\n",
      "loss:  0.28720906376838684 accuracy:  1.0 \n",
      "\n",
      "Epoch  53\n",
      "loss:  0.2815374433994293 accuracy:  1.0 \n",
      "\n",
      "Epoch  54\n",
      "loss:  0.28177040815353394 accuracy:  1.0 \n",
      "\n",
      "Epoch  55\n",
      "loss:  0.31109383702278137 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  56\n",
      "loss:  0.2752479612827301 accuracy:  1.0 \n",
      "\n",
      "Epoch  57\n",
      "loss:  0.2600499093532562 accuracy:  1.0 \n",
      "\n",
      "Epoch  58\n",
      "loss:  0.29670193791389465 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  59\n",
      "loss:  0.2908288836479187 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  60\n",
      "loss:  0.2554031014442444 accuracy:  1.0 \n",
      "\n",
      "Epoch  61\n",
      "loss:  0.25890377163887024 accuracy:  1.0 \n",
      "\n",
      "Epoch  62\n",
      "loss:  0.26755622029304504 accuracy:  1.0 \n",
      "\n",
      "Epoch  63\n",
      "loss:  0.263576477766037 accuracy:  1.0 \n",
      "\n",
      "Epoch  64\n",
      "loss:  0.26845458149909973 accuracy:  1.0 \n",
      "\n",
      "Epoch  65\n",
      "loss:  0.2523816227912903 accuracy:  1.0 \n",
      "\n",
      "Epoch  66\n",
      "loss:  0.27062544226646423 accuracy:  1.0 \n",
      "\n",
      "Epoch  67\n",
      "loss:  0.2528040111064911 accuracy:  1.0 \n",
      "\n",
      "Epoch  68\n",
      "loss:  0.25815215706825256 accuracy:  1.0 \n",
      "\n",
      "Epoch  69\n",
      "loss:  0.2362780123949051 accuracy:  1.0 \n",
      "\n",
      "Epoch  70\n",
      "loss:  0.25379613041877747 accuracy:  1.0 \n",
      "\n",
      "Epoch  71\n",
      "loss:  0.2722008526325226 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  72\n",
      "loss:  0.2259434163570404 accuracy:  1.0 \n",
      "\n",
      "Epoch  73\n",
      "loss:  0.25246018171310425 accuracy:  1.0 \n",
      "\n",
      "Epoch  74\n",
      "loss:  0.2320970594882965 accuracy:  1.0 \n",
      "\n",
      "Epoch  75\n",
      "loss:  0.2468133270740509 accuracy:  1.0 \n",
      "\n",
      "Epoch  76\n",
      "loss:  0.24496521055698395 accuracy:  1.0 \n",
      "\n",
      "Epoch  77\n",
      "loss:  0.2358025163412094 accuracy:  1.0 \n",
      "\n",
      "Epoch  78\n",
      "loss:  0.22878043353557587 accuracy:  1.0 \n",
      "\n",
      "Epoch  79\n",
      "loss:  0.236357182264328 accuracy:  1.0 \n",
      "\n",
      "Epoch  80\n",
      "loss:  0.22485850751399994 accuracy:  1.0 \n",
      "\n",
      "Epoch  81\n",
      "loss:  0.2223059982061386 accuracy:  1.0 \n",
      "\n",
      "Epoch  82\n",
      "loss:  0.2434319108724594 accuracy:  1.0 \n",
      "\n",
      "Epoch  83\n",
      "loss:  0.22387012839317322 accuracy:  1.0 \n",
      "\n",
      "Epoch  84\n",
      "loss:  0.20722989737987518 accuracy:  1.0 \n",
      "\n",
      "Epoch  85\n",
      "loss:  0.2214973121881485 accuracy:  1.0 \n",
      "\n",
      "Epoch  86\n",
      "loss:  0.23813946545124054 accuracy:  1.0 \n",
      "\n",
      "Epoch  87\n",
      "loss:  0.22874464094638824 accuracy:  1.0 \n",
      "\n",
      "Epoch  88\n",
      "loss:  0.1977100968360901 accuracy:  1.0 \n",
      "\n",
      "Epoch  89\n",
      "loss:  0.21776103973388672 accuracy:  1.0 \n",
      "\n",
      "Epoch  90\n",
      "loss:  0.2348424345254898 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  91\n",
      "loss:  0.19848142564296722 accuracy:  1.0 \n",
      "\n",
      "Epoch  92\n",
      "loss:  0.20985053479671478 accuracy:  1.0 \n",
      "\n",
      "Epoch  93\n",
      "loss:  0.22112013399600983 accuracy:  1.0 \n",
      "\n",
      "Epoch  94\n",
      "loss:  0.19812805950641632 accuracy:  1.0 \n",
      "\n",
      "Epoch  95\n",
      "loss:  0.21295122802257538 accuracy:  1.0 \n",
      "\n",
      "Epoch  96\n",
      "loss:  0.20476661622524261 accuracy:  1.0 \n",
      "\n",
      "Epoch  97\n",
      "loss:  0.2033894807100296 accuracy:  1.0 \n",
      "\n",
      "Epoch  98\n",
      "loss:  0.19180241227149963 accuracy:  1.0 \n",
      "\n",
      "Epoch  99\n",
      "loss:  0.19858138263225555 accuracy:  1.0 \n",
      "\n",
      "Epoch  100\n",
      "loss:  0.1975315362215042 accuracy:  1.0 \n",
      "\n",
      "Epoch  101\n",
      "loss:  0.1969396024942398 accuracy:  1.0 \n",
      "\n",
      "Epoch  102\n",
      "loss:  0.20347575843334198 accuracy:  1.0 \n",
      "\n",
      "Epoch  103\n",
      "loss:  0.2017539143562317 accuracy:  1.0 \n",
      "\n",
      "Epoch  104\n",
      "loss:  0.19419044256210327 accuracy:  1.0 \n",
      "\n",
      "Epoch  105\n",
      "loss:  0.19286595284938812 accuracy:  1.0 \n",
      "\n",
      "Epoch  106\n",
      "loss:  0.17589598894119263 accuracy:  1.0 \n",
      "\n",
      "Epoch  107\n",
      "loss:  0.19101563096046448 accuracy:  1.0 \n",
      "\n",
      "Epoch  108\n",
      "loss:  0.20004399120807648 accuracy:  1.0 \n",
      "\n",
      "Epoch  109\n",
      "loss:  0.18916650116443634 accuracy:  1.0 \n",
      "\n",
      "Epoch  110\n",
      "loss:  0.18453191220760345 accuracy:  1.0 \n",
      "\n",
      "Epoch  111\n",
      "loss:  0.18793848156929016 accuracy:  1.0 \n",
      "\n",
      "Epoch  112\n",
      "loss:  0.18233250081539154 accuracy:  1.0 \n",
      "\n",
      "Epoch  113\n",
      "loss:  0.17481644451618195 accuracy:  1.0 \n",
      "\n",
      "Epoch  114\n",
      "loss:  0.18028190732002258 accuracy:  1.0 \n",
      "\n",
      "Epoch  115\n",
      "loss:  0.18548566102981567 accuracy:  1.0 \n",
      "\n",
      "Epoch  116\n",
      "loss:  0.17495250701904297 accuracy:  1.0 \n",
      "\n",
      "Epoch  117\n",
      "loss:  0.1694183498620987 accuracy:  1.0 \n",
      "\n",
      "Epoch  118\n",
      "loss:  0.18000276386737823 accuracy:  1.0 \n",
      "\n",
      "Epoch  119\n",
      "loss:  0.18183551728725433 accuracy:  1.0 \n",
      "\n",
      "Epoch  120\n",
      "loss:  0.1757056564092636 accuracy:  1.0 \n",
      "\n",
      "Epoch  121\n",
      "loss:  0.16855880618095398 accuracy:  1.0 \n",
      "\n",
      "Epoch  122\n",
      "loss:  0.19251208007335663 accuracy:  1.0 \n",
      "\n",
      "Epoch  123\n",
      "loss:  0.17123636603355408 accuracy:  1.0 \n",
      "\n",
      "Epoch  124\n",
      "loss:  0.16629768908023834 accuracy:  1.0 \n",
      "\n",
      "Epoch  125\n",
      "loss:  0.17500486969947815 accuracy:  1.0 \n",
      "\n",
      "Epoch  126\n",
      "loss:  0.16642342507839203 accuracy:  1.0 \n",
      "\n",
      "Epoch  127\n",
      "loss:  0.17364101111888885 accuracy:  1.0 \n",
      "\n",
      "Epoch  128\n",
      "loss:  0.16463394463062286 accuracy:  1.0 \n",
      "\n",
      "Epoch  129\n",
      "loss:  0.15717346966266632 accuracy:  1.0 \n",
      "\n",
      "Epoch  130\n",
      "loss:  0.1653306782245636 accuracy:  1.0 \n",
      "\n",
      "Epoch  131\n",
      "loss:  0.16955898702144623 accuracy:  1.0 \n",
      "\n",
      "Epoch  132\n",
      "loss:  0.1617862433195114 accuracy:  1.0 \n",
      "\n",
      "Epoch  133\n",
      "loss:  0.16798149049282074 accuracy:  1.0 \n",
      "\n",
      "Epoch  134\n",
      "loss:  0.15820607542991638 accuracy:  1.0 \n",
      "\n",
      "Epoch  135\n",
      "loss:  0.16607049107551575 accuracy:  1.0 \n",
      "\n",
      "Epoch  136\n",
      "loss:  0.1507723331451416 accuracy:  1.0 \n",
      "\n",
      "Epoch  137\n",
      "loss:  0.15349942445755005 accuracy:  1.0 \n",
      "\n",
      "Epoch  138\n",
      "loss:  0.1613302081823349 accuracy:  1.0 \n",
      "\n",
      "Epoch  139\n",
      "loss:  0.16318994760513306 accuracy:  1.0 \n",
      "\n",
      "Epoch  140\n",
      "loss:  0.1467958241701126 accuracy:  1.0 \n",
      "\n",
      "Epoch  141\n",
      "loss:  0.15853431820869446 accuracy:  1.0 \n",
      "\n",
      "Epoch  142\n",
      "loss:  0.1549190878868103 accuracy:  1.0 \n",
      "\n",
      "Epoch  143\n",
      "loss:  0.14730305969715118 accuracy:  1.0 \n",
      "\n",
      "Epoch  144\n",
      "loss:  0.14448601007461548 accuracy:  1.0 \n",
      "\n",
      "Epoch  145\n",
      "loss:  0.16448338329792023 accuracy:  1.0 \n",
      "\n",
      "Epoch  146\n",
      "loss:  0.14802922308444977 accuracy:  1.0 \n",
      "\n",
      "Epoch  147\n",
      "loss:  0.14798472821712494 accuracy:  1.0 \n",
      "\n",
      "Epoch  148\n",
      "loss:  0.14350436627864838 accuracy:  1.0 \n",
      "\n",
      "Epoch  149\n",
      "loss:  0.14465461671352386 accuracy:  1.0 \n",
      "\n",
      "Epoch  150\n",
      "loss:  0.15100309252738953 accuracy:  1.0 \n",
      "\n",
      "Epoch  151\n",
      "loss:  0.14665919542312622 accuracy:  1.0 \n",
      "\n",
      "Epoch  152\n",
      "loss:  0.1455366015434265 accuracy:  1.0 \n",
      "\n",
      "Epoch  153\n",
      "loss:  0.14561809599399567 accuracy:  1.0 \n",
      "\n",
      "Epoch  154\n",
      "loss:  0.1473177820444107 accuracy:  1.0 \n",
      "\n",
      "Epoch  155\n",
      "loss:  0.13921292126178741 accuracy:  1.0 \n",
      "\n",
      "Epoch  156\n",
      "loss:  0.15001462399959564 accuracy:  1.0 \n",
      "\n",
      "Epoch  157\n",
      "loss:  0.13848501443862915 accuracy:  1.0 \n",
      "\n",
      "Epoch  158\n",
      "loss:  0.13621075451374054 accuracy:  1.0 \n",
      "\n",
      "Epoch  159\n",
      "loss:  0.14292536675930023 accuracy:  1.0 \n",
      "\n",
      "Epoch  160\n",
      "loss:  0.13980458676815033 accuracy:  1.0 \n",
      "\n",
      "Epoch  161\n",
      "loss:  0.1422930806875229 accuracy:  1.0 \n",
      "\n",
      "Epoch  162\n",
      "loss:  0.14278477430343628 accuracy:  1.0 \n",
      "\n",
      "Epoch  163\n",
      "loss:  0.13445840775966644 accuracy:  1.0 \n",
      "\n",
      "Epoch  164\n",
      "loss:  0.14193683862686157 accuracy:  1.0 \n",
      "\n",
      "Epoch  165\n",
      "loss:  0.1300976723432541 accuracy:  1.0 \n",
      "\n",
      "Epoch  166\n",
      "loss:  0.1414945423603058 accuracy:  1.0 \n",
      "\n",
      "Epoch  167\n",
      "loss:  0.13087716698646545 accuracy:  1.0 \n",
      "\n",
      "Epoch  168\n",
      "loss:  0.13150036334991455 accuracy:  1.0 \n",
      "\n",
      "Epoch  169\n",
      "loss:  0.12798188626766205 accuracy:  1.0 \n",
      "\n",
      "Epoch  170\n",
      "loss:  0.14051184058189392 accuracy:  1.0 \n",
      "\n",
      "Epoch  171\n",
      "loss:  0.13431426882743835 accuracy:  1.0 \n",
      "\n",
      "Epoch  172\n",
      "loss:  0.1348845213651657 accuracy:  1.0 \n",
      "\n",
      "Epoch  173\n",
      "loss:  0.13051439821720123 accuracy:  1.0 \n",
      "\n",
      "Epoch  174\n",
      "loss:  0.12649887800216675 accuracy:  1.0 \n",
      "\n",
      "Epoch  175\n",
      "loss:  0.13266518712043762 accuracy:  1.0 \n",
      "\n",
      "Epoch  176\n",
      "loss:  0.14493821561336517 accuracy:  1.0 \n",
      "\n",
      "Epoch  177\n",
      "loss:  0.12625278532505035 accuracy:  1.0 \n",
      "\n",
      "Epoch  178\n",
      "loss:  0.12662480771541595 accuracy:  1.0 \n",
      "\n",
      "Epoch  179\n",
      "loss:  0.12554237246513367 accuracy:  1.0 \n",
      "\n",
      "Epoch  180\n",
      "loss:  0.13294266164302826 accuracy:  1.0 \n",
      "\n",
      "Epoch  181\n",
      "loss:  0.12639793753623962 accuracy:  1.0 \n",
      "\n",
      "Epoch  182\n",
      "loss:  0.12281732261180878 accuracy:  1.0 \n",
      "\n",
      "Epoch  183\n",
      "loss:  0.12410164624452591 accuracy:  1.0 \n",
      "\n",
      "Epoch  184\n",
      "loss:  0.13546861708164215 accuracy:  1.0 \n",
      "\n",
      "Epoch  185\n",
      "loss:  0.1251433938741684 accuracy:  1.0 \n",
      "\n",
      "Epoch  186\n",
      "loss:  0.12015509605407715 accuracy:  1.0 \n",
      "\n",
      "Epoch  187\n",
      "loss:  0.12365113198757172 accuracy:  1.0 \n",
      "\n",
      "Epoch  188\n",
      "loss:  0.12028953433036804 accuracy:  1.0 \n",
      "\n",
      "Epoch  189\n",
      "loss:  0.13034020364284515 accuracy:  1.0 \n",
      "\n",
      "Epoch  190\n",
      "loss:  0.12590600550174713 accuracy:  1.0 \n",
      "\n",
      "Epoch  191\n",
      "loss:  0.11603475362062454 accuracy:  1.0 \n",
      "\n",
      "Epoch  192\n",
      "loss:  0.11557584255933762 accuracy:  1.0 \n",
      "\n",
      "Epoch  193\n",
      "loss:  0.12156311422586441 accuracy:  1.0 \n",
      "\n",
      "Epoch  194\n",
      "loss:  0.12502257525920868 accuracy:  1.0 \n",
      "\n",
      "Epoch  195\n",
      "loss:  0.11487121880054474 accuracy:  1.0 \n",
      "\n",
      "Epoch  196\n",
      "loss:  0.11762893199920654 accuracy:  1.0 \n",
      "\n",
      "Epoch  197\n",
      "loss:  0.11274037510156631 accuracy:  1.0 \n",
      "\n",
      "Epoch  198\n",
      "loss:  0.11773279309272766 accuracy:  1.0 \n",
      "\n",
      "Epoch  199\n",
      "loss:  0.12347175925970078 accuracy:  1.0 \n",
      "\n",
      "Evaluation result: Loss:  0.05999784544110298  Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "data_all = torch.utils.data.TensorDataset(input_columns_all, output_columns)\n",
    "\n",
    "#train_split, val_split and test_split defined earlier\n",
    "train_set_all, val_set_all, test_set_all = torch.utils.data.random_split(data_all, [train_split, val_split, test_split])\n",
    "\n",
    "train_loader_all = torch.utils.data.DataLoader(train_set_all, 16, shuffle = True)\n",
    "val_loader_all = torch.utils.data.DataLoader(val_set_all)\n",
    "test_loader_all = torch.utils.data.DataLoader(test_set_all)\n",
    "\n",
    "model_all = LogisticRegression(4, len(species))\n",
    "history_all = fit(model_all, train_loader_all, val_loader_all, epochs, learning_rate)\n",
    "loss , accuracy = evaluate(model_all, test_loader_all)\n",
    "print(\"Evaluation result: Loss: \", loss.item(), \" Accuracy: \", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}