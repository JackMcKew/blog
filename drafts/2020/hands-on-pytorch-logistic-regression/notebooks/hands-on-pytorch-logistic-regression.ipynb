{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "In this post we'll go through a few things typical for any project using machine learning:\n",
    "\n",
    "1. Data exploration & analysis\n",
    "2. Build a model\n",
    "3. Train the model\n",
    "4. Evaluate the model\n",
    "\n",
    "While this is a very high level overview of what we're about to do. This process is almost the same in any size & complex machine learning project.\n",
    "\n",
    "We'll be using the iris dataset, which is a very famous hello world dataset in machine learning which contains 4 parameters which describe three species of flower (the iris flower). The sepal length & width, and the petal length & width are provided for 50 samples of each species. The sepals of a lower are the green leafy parts surrounding the flower head (which has the petals).\n",
    "\n",
    "We'll also be trying to classify flowers into their species from their attributes using the logistic regression model. Logistic regression predicts the probability that something is either one thing or not based on the input variables.\n",
    "\n",
    "As always, we begin by importing the neccessary libraries/packages."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from IPython.core.display import HTML\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## Data Exploration & Analysis\n",
    "\n",
    "Time to dive in and take a lot at what we'll be working with. If this was a very rigourous project, we'd split the dataset into our train/validate/test set before even starting the validation as the modeller should never know what the test set looks like otherwise it introduces bias. Nevertheless in the pursuit of brevity we'll not do this here.\n",
    "\n",
    "First off we'll take a peek at the first 5 rows of data so we can at least see what type of data they are (they are given as floats or numbers with decimal points) and the species is defined as a string. We can then check how many samples of each species we've been provided with the `value_counts()` method and finally we can take a look with describe to see common statistics about each attribute of our pretty little flowers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   sepal.length  sepal.width  petal.length  petal.width variety\n0           5.1          3.5           1.4          0.2  Setosa\n1           4.9          3.0           1.4          0.2  Setosa\n2           4.7          3.2           1.3          0.2  Setosa\n3           4.6          3.1           1.5          0.2  Setosa\n4           5.0          3.6           1.4          0.2  Setosa\nVirginica     50\nSetosa        50\nVersicolor    50\nName: variety, dtype: int64\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       sepal.length  sepal.width  petal.length  petal.width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.057333      3.758000     1.199333\n",
       "std        0.828066     0.435866      1.765298     0.762238\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal.length</th>\n      <th>sepal.width</th>\n      <th>petal.length</th>\n      <th>petal.width</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.843333</td>\n      <td>3.057333</td>\n      <td>3.758000</td>\n      <td>1.199333</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.828066</td>\n      <td>0.435866</td>\n      <td>1.765298</td>\n      <td>0.762238</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.300000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.100000</td>\n      <td>2.800000</td>\n      <td>1.600000</td>\n      <td>0.300000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.800000</td>\n      <td>3.000000</td>\n      <td>4.350000</td>\n      <td>1.300000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.400000</td>\n      <td>3.300000</td>\n      <td>5.100000</td>\n      <td>1.800000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.900000</td>\n      <td>4.400000</td>\n      <td>6.900000</td>\n      <td>2.500000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "iris = pd.read_csv('iris.csv')\n",
    "print(iris.head())\n",
    "print(iris['variety'].value_counts())\n",
    "iris.describe()"
   ]
  },
  {
   "source": [
    "Next we'll extract our species into a list for use later on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Setosa', 'Versicolor', 'Virginica']\n"
     ]
    }
   ],
   "source": [
    "species = list(iris[\"variety\"].unique())\n",
    "\n",
    "print(species)"
   ]
  },
  {
   "source": [
    "Since the iris dataset has 4 input, and a single output feature, when we create a visualisation of all of this it'll be a 5 dimensional plot, and we'll use a scatter plot to represent this. By using the `plotly` package we can also make it interactive which is super handy for our 3D minds to visualise higher dimensional things."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": "<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>                <div id=\"25df0faf-abc6-4eb9-ac8e-e4ca59d79153\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"25df0faf-abc6-4eb9-ac8e-e4ca59d79153\")) {                    Plotly.newPlot(                        \"25df0faf-abc6-4eb9-ac8e-e4ca59d79153\",                        [{\"hovertemplate\": \"variety=Setosa<br>sepal.length=%{x}<br>sepal.width=%{y}<br>petal.width=%{z}<br>petal.length=%{marker.size}<extra></extra>\", \"legendgroup\": \"Setosa\", \"marker\": {\"color\": \"#636efa\", \"opacity\": 0.7, \"size\": [1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1.5, 1.7, 1.5, 1.0, 1.7, 1.9, 1.6, 1.6, 1.5, 1.4, 1.6, 1.6, 1.5, 1.5, 1.4, 1.5, 1.2, 1.3, 1.4, 1.3, 1.5, 1.3, 1.3, 1.3, 1.6, 1.9, 1.4, 1.6, 1.4, 1.5, 1.4], \"sizemode\": \"area\", \"sizeref\": 0.01725, \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"Setosa\", \"scene\": \"scene\", \"showlegend\": true, \"type\": \"scatter3d\", \"x\": [5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 5.4, 5.1, 4.6, 5.1, 4.8, 5.0, 5.0, 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, 5.5, 4.9, 5.0, 5.5, 4.9, 4.4, 5.1, 5.0, 4.5, 4.4, 5.0, 5.1, 4.8, 5.1, 4.6, 5.3, 5.0], \"y\": [3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3.0, 3.0, 4.0, 4.4, 3.9, 3.5, 3.8, 3.8, 3.4, 3.7, 3.6, 3.3, 3.4, 3.0, 3.4, 3.5, 3.4, 3.2, 3.1, 3.4, 4.1, 4.2, 3.1, 3.2, 3.5, 3.6, 3.0, 3.4, 3.5, 2.3, 3.2, 3.5, 3.8, 3.0, 3.8, 3.2, 3.7, 3.3], \"z\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3, 0.2, 0.4, 0.2, 0.5, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 0.2, 0.2, 0.3, 0.3, 0.2, 0.6, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2]}, {\"hovertemplate\": \"variety=Versicolor<br>sepal.length=%{x}<br>sepal.width=%{y}<br>petal.width=%{z}<br>petal.length=%{marker.size}<extra></extra>\", \"legendgroup\": \"Versicolor\", \"marker\": {\"color\": \"#EF553B\", \"opacity\": 0.7, \"size\": [4.7, 4.5, 4.9, 4.0, 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4.0, 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4.0, 4.9, 4.7, 4.3, 4.4, 4.8, 5.0, 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1, 4.0, 4.4, 4.6, 4.0, 3.3, 4.2, 4.2, 4.2, 4.3, 3.0, 4.1], \"sizemode\": \"area\", \"sizeref\": 0.01725, \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"Versicolor\", \"scene\": \"scene\", \"showlegend\": true, \"type\": \"scatter3d\", \"x\": [7.0, 6.4, 6.9, 5.5, 6.5, 5.7, 6.3, 4.9, 6.6, 5.2, 5.0, 5.9, 6.0, 6.1, 5.6, 6.7, 5.6, 5.8, 6.2, 5.6, 5.9, 6.1, 6.3, 6.1, 6.4, 6.6, 6.8, 6.7, 6.0, 5.7, 5.5, 5.5, 5.8, 6.0, 5.4, 6.0, 6.7, 6.3, 5.6, 5.5, 5.5, 6.1, 5.8, 5.0, 5.6, 5.7, 5.7, 6.2, 5.1, 5.7], \"y\": [3.2, 3.2, 3.1, 2.3, 2.8, 2.8, 3.3, 2.4, 2.9, 2.7, 2.0, 3.0, 2.2, 2.9, 2.9, 3.1, 3.0, 2.7, 2.2, 2.5, 3.2, 2.8, 2.5, 2.8, 2.9, 3.0, 2.8, 3.0, 2.9, 2.6, 2.4, 2.4, 2.7, 2.7, 3.0, 3.4, 3.1, 2.3, 3.0, 2.5, 2.6, 3.0, 2.6, 2.3, 2.7, 3.0, 2.9, 2.9, 2.5, 2.8], \"z\": [1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1.0, 1.3, 1.4, 1.0, 1.5, 1.0, 1.4, 1.3, 1.4, 1.5, 1.0, 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7, 1.5, 1.0, 1.1, 1.0, 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2, 1.4, 1.2, 1.0, 1.3, 1.2, 1.3, 1.3, 1.1, 1.3]}, {\"hovertemplate\": \"variety=Virginica<br>sepal.length=%{x}<br>sepal.width=%{y}<br>petal.width=%{z}<br>petal.length=%{marker.size}<extra></extra>\", \"legendgroup\": \"Virginica\", \"marker\": {\"color\": \"#00cc96\", \"opacity\": 0.7, \"size\": [6.0, 5.1, 5.9, 5.6, 5.8, 6.6, 4.5, 6.3, 5.8, 6.1, 5.1, 5.3, 5.5, 5.0, 5.1, 5.3, 5.5, 6.7, 6.9, 5.0, 5.7, 4.9, 6.7, 4.9, 5.7, 6.0, 4.8, 4.9, 5.6, 5.8, 6.1, 6.4, 5.6, 5.1, 5.6, 6.1, 5.6, 5.5, 4.8, 5.4, 5.6, 5.1, 5.1, 5.9, 5.7, 5.2, 5.0, 5.2, 5.4, 5.1], \"sizemode\": \"area\", \"sizeref\": 0.01725, \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"Virginica\", \"scene\": \"scene\", \"showlegend\": true, \"type\": \"scatter3d\", \"x\": [6.3, 5.8, 7.1, 6.3, 6.5, 7.6, 4.9, 7.3, 6.7, 7.2, 6.5, 6.4, 6.8, 5.7, 5.8, 6.4, 6.5, 7.7, 7.7, 6.0, 6.9, 5.6, 7.7, 6.3, 6.7, 7.2, 6.2, 6.1, 6.4, 7.2, 7.4, 7.9, 6.4, 6.3, 6.1, 7.7, 6.3, 6.4, 6.0, 6.9, 6.7, 6.9, 5.8, 6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9], \"y\": [3.3, 2.7, 3.0, 2.9, 3.0, 3.0, 2.5, 2.9, 2.5, 3.6, 3.2, 2.7, 3.0, 2.5, 2.8, 3.2, 3.0, 3.8, 2.6, 2.2, 3.2, 2.8, 2.8, 2.7, 3.3, 3.2, 2.8, 3.0, 2.8, 3.0, 2.8, 3.8, 2.8, 2.8, 2.6, 3.0, 3.4, 3.1, 3.0, 3.1, 3.1, 3.1, 2.7, 3.2, 3.3, 3.0, 2.5, 3.0, 3.4, 3.0], \"z\": [2.5, 1.9, 2.1, 1.8, 2.2, 2.1, 1.7, 1.8, 1.8, 2.5, 2.0, 1.9, 2.1, 2.0, 2.4, 2.3, 1.8, 2.2, 2.3, 1.5, 2.3, 2.0, 2.0, 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6, 1.9, 2.0, 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9, 2.3, 2.5, 2.3, 1.9, 2.0, 2.3, 1.8]}],                        {\"legend\": {\"itemsizing\": \"constant\", \"title\": {\"text\": \"variety\"}, \"tracegroupgap\": 0}, \"margin\": {\"b\": 0, \"l\": 0, \"r\": 0, \"t\": 0}, \"scene\": {\"domain\": {\"x\": [0.0, 1.0], \"y\": [0.0, 1.0]}, \"xaxis\": {\"title\": {\"text\": \"sepal.length\"}}, \"yaxis\": {\"title\": {\"text\": \"sepal.width\"}}, \"zaxis\": {\"title\": {\"text\": \"petal.width\"}}}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n</body>\n</html>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "fig = px.scatter_3d(iris[[\"sepal.length\",\"sepal.width\",\"petal.length\",\"petal.width\",\"variety\"]],\n",
    "                    x = 'sepal.length',\n",
    "                    y = 'sepal.width',\n",
    "                    z = 'petal.width',\n",
    "                    size = 'petal.length',\n",
    "                    color = 'variety',\n",
    "                    opacity = 0.7)\n",
    "\n",
    "fig.update_layout(margin = dict(l=0, r=0, b=0, t=0))\n",
    "\n",
    "HTML(plotly.offline.plot(fig, filename='5d_iris_scatter.html',include_plotlyjs='cdn'))"
   ]
  },
  {
   "source": [
    "To try and make it easier for our 3D minds, let's just plot all the dimensions in a single plot so we can see which features correlate with each other. To do this, we'll use a scatter matrix (also known as a pairwise plot)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": "<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>                <div id=\"e4408c60-91fc-421d-a3b6-a66fddc8fb94\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e4408c60-91fc-421d-a3b6-a66fddc8fb94\")) {                    Plotly.newPlot(                        \"e4408c60-91fc-421d-a3b6-a66fddc8fb94\",                        [{\"dimensions\": [{\"axis\": {\"matches\": true}, \"label\": \"sepal.width\", \"values\": [3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3.0, 3.0, 4.0, 4.4, 3.9, 3.5, 3.8, 3.8, 3.4, 3.7, 3.6, 3.3, 3.4, 3.0, 3.4, 3.5, 3.4, 3.2, 3.1, 3.4, 4.1, 4.2, 3.1, 3.2, 3.5, 3.6, 3.0, 3.4, 3.5, 2.3, 3.2, 3.5, 3.8, 3.0, 3.8, 3.2, 3.7, 3.3]}, {\"axis\": {\"matches\": true}, \"label\": \"sepal.length\", \"values\": [5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 5.4, 5.1, 4.6, 5.1, 4.8, 5.0, 5.0, 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, 5.5, 4.9, 5.0, 5.5, 4.9, 4.4, 5.1, 5.0, 4.5, 4.4, 5.0, 5.1, 4.8, 5.1, 4.6, 5.3, 5.0]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.width\", \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3, 0.2, 0.4, 0.2, 0.5, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 0.2, 0.2, 0.3, 0.3, 0.2, 0.6, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.length\", \"values\": [1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1.5, 1.7, 1.5, 1.0, 1.7, 1.9, 1.6, 1.6, 1.5, 1.4, 1.6, 1.6, 1.5, 1.5, 1.4, 1.5, 1.2, 1.3, 1.4, 1.3, 1.5, 1.3, 1.3, 1.3, 1.6, 1.9, 1.4, 1.6, 1.4, 1.5, 1.4]}], \"hovertemplate\": \"variety=Setosa<br>%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<extra></extra>\", \"legendgroup\": \"Setosa\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"name\": \"Setosa\", \"showlegend\": true, \"type\": \"splom\"}, {\"dimensions\": [{\"axis\": {\"matches\": true}, \"label\": \"sepal.width\", \"values\": [3.2, 3.2, 3.1, 2.3, 2.8, 2.8, 3.3, 2.4, 2.9, 2.7, 2.0, 3.0, 2.2, 2.9, 2.9, 3.1, 3.0, 2.7, 2.2, 2.5, 3.2, 2.8, 2.5, 2.8, 2.9, 3.0, 2.8, 3.0, 2.9, 2.6, 2.4, 2.4, 2.7, 2.7, 3.0, 3.4, 3.1, 2.3, 3.0, 2.5, 2.6, 3.0, 2.6, 2.3, 2.7, 3.0, 2.9, 2.9, 2.5, 2.8]}, {\"axis\": {\"matches\": true}, \"label\": \"sepal.length\", \"values\": [7.0, 6.4, 6.9, 5.5, 6.5, 5.7, 6.3, 4.9, 6.6, 5.2, 5.0, 5.9, 6.0, 6.1, 5.6, 6.7, 5.6, 5.8, 6.2, 5.6, 5.9, 6.1, 6.3, 6.1, 6.4, 6.6, 6.8, 6.7, 6.0, 5.7, 5.5, 5.5, 5.8, 6.0, 5.4, 6.0, 6.7, 6.3, 5.6, 5.5, 5.5, 6.1, 5.8, 5.0, 5.6, 5.7, 5.7, 6.2, 5.1, 5.7]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.width\", \"values\": [1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1.0, 1.3, 1.4, 1.0, 1.5, 1.0, 1.4, 1.3, 1.4, 1.5, 1.0, 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7, 1.5, 1.0, 1.1, 1.0, 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2, 1.4, 1.2, 1.0, 1.3, 1.2, 1.3, 1.3, 1.1, 1.3]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.length\", \"values\": [4.7, 4.5, 4.9, 4.0, 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4.0, 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4.0, 4.9, 4.7, 4.3, 4.4, 4.8, 5.0, 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1, 4.0, 4.4, 4.6, 4.0, 3.3, 4.2, 4.2, 4.2, 4.3, 3.0, 4.1]}], \"hovertemplate\": \"variety=Versicolor<br>%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<extra></extra>\", \"legendgroup\": \"Versicolor\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"name\": \"Versicolor\", \"showlegend\": true, \"type\": \"splom\"}, {\"dimensions\": [{\"axis\": {\"matches\": true}, \"label\": \"sepal.width\", \"values\": [3.3, 2.7, 3.0, 2.9, 3.0, 3.0, 2.5, 2.9, 2.5, 3.6, 3.2, 2.7, 3.0, 2.5, 2.8, 3.2, 3.0, 3.8, 2.6, 2.2, 3.2, 2.8, 2.8, 2.7, 3.3, 3.2, 2.8, 3.0, 2.8, 3.0, 2.8, 3.8, 2.8, 2.8, 2.6, 3.0, 3.4, 3.1, 3.0, 3.1, 3.1, 3.1, 2.7, 3.2, 3.3, 3.0, 2.5, 3.0, 3.4, 3.0]}, {\"axis\": {\"matches\": true}, \"label\": \"sepal.length\", \"values\": [6.3, 5.8, 7.1, 6.3, 6.5, 7.6, 4.9, 7.3, 6.7, 7.2, 6.5, 6.4, 6.8, 5.7, 5.8, 6.4, 6.5, 7.7, 7.7, 6.0, 6.9, 5.6, 7.7, 6.3, 6.7, 7.2, 6.2, 6.1, 6.4, 7.2, 7.4, 7.9, 6.4, 6.3, 6.1, 7.7, 6.3, 6.4, 6.0, 6.9, 6.7, 6.9, 5.8, 6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.width\", \"values\": [2.5, 1.9, 2.1, 1.8, 2.2, 2.1, 1.7, 1.8, 1.8, 2.5, 2.0, 1.9, 2.1, 2.0, 2.4, 2.3, 1.8, 2.2, 2.3, 1.5, 2.3, 2.0, 2.0, 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6, 1.9, 2.0, 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9, 2.3, 2.5, 2.3, 1.9, 2.0, 2.3, 1.8]}, {\"axis\": {\"matches\": true}, \"label\": \"petal.length\", \"values\": [6.0, 5.1, 5.9, 5.6, 5.8, 6.6, 4.5, 6.3, 5.8, 6.1, 5.1, 5.3, 5.5, 5.0, 5.1, 5.3, 5.5, 6.7, 6.9, 5.0, 5.7, 4.9, 6.7, 4.9, 5.7, 6.0, 4.8, 4.9, 5.6, 5.8, 6.1, 6.4, 5.6, 5.1, 5.6, 6.1, 5.6, 5.5, 4.8, 5.4, 5.6, 5.1, 5.1, 5.9, 5.7, 5.2, 5.0, 5.2, 5.4, 5.1]}], \"hovertemplate\": \"variety=Virginica<br>%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<extra></extra>\", \"legendgroup\": \"Virginica\", \"marker\": {\"color\": \"#00cc96\", \"symbol\": \"circle\"}, \"name\": \"Virginica\", \"showlegend\": true, \"type\": \"splom\"}],                        {\"dragmode\": \"select\", \"legend\": {\"title\": {\"text\": \"variety\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n</body>\n</html>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "fig = px.scatter_matrix(iris, dimensions=[\"sepal.width\", \"sepal.length\", \"petal.width\", \"petal.length\"],color=\"variety\")\n",
    "HTML(plotly.offline.plot(fig, filename='5d_scatter_matrix.html',include_plotlyjs='cdn'))"
   ]
  },
  {
   "source": [
    "By looking at this plots we can see that one species of flower is well separated from the others, while the others are mixed and overlapped for every pair of features. But if we use `petal length and width` that'll give us a nice separation between the species, and we'll train our first model on specifically these two features. This is known as feature selection.\n",
    "\n",
    "## Building the Model\n",
    "\n",
    "Now it's time to prepare the data for our model, and create the model. We do this by creating tensors of the input and output data. A tensor is a general term for data with any amount of dimesion. A 2D tensor is commonly referred to as a vector, and a 1D tensor as a scalar, if that helps to think of these things as. Almost everything we'll come across in the world of machine learning makes use of tensors in one form or another."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input columns all:  torch.Size([150, 4]) torch.float32\nInput columns:  torch.Size([150, 2]) torch.float32\nOutput columns:  torch.Size([150]) torch.int8\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['petal.length', 'petal.width']\n",
    "\n",
    "input_columns_all = torch.from_numpy(iris[list(iris.select_dtypes('float').columns)].to_numpy()).type(torch.float32)\n",
    "input_columns = torch.from_numpy(iris[selected_features].to_numpy()).type(torch.float32)\n",
    "output_columns = torch.tensor(iris['variety'].astype('category').cat.codes)\n",
    "\n",
    "print(\"Input columns all: \", input_columns_all.shape, input_columns_all.dtype)\n",
    "print(\"Input columns: \", input_columns.shape, input_columns.dtype)\n",
    "print(\"Output columns: \", output_columns.shape, output_columns.dtype)\n",
    "\n",
    "data = torch.utils.data.TensorDataset(input_columns, output_columns)"
   ]
  },
  {
   "source": [
    "As mentioned before, it's time to split up the dataset into a train, validation and test set. We'll use a split of 70-20-10 for these and make use of the `random_split` tool in PyTorch to randomize which records fall into each set.\n",
    "\n",
    "Since we're using PyTorch we need to make this data iterable using `DataLoader`, and to enable many other features like shuffling, mapping and more. This also enables us to generate batches to train on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.1\n",
    "rows = iris.shape[0]\n",
    "test_split = int(rows*split)\n",
    "val_split = int(rows*split*2)\n",
    "train_split = rows - val_split - test_split\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(data, [train_split, val_split, test_split])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set) \n",
    "test_loader = torch.utils.data.DataLoader(test_set)"
   ]
  },
  {
   "source": [
    "Finally let's create the model! We do this by extending the `Module` class from within PyTorch, from the perspective of neural networks this could be seen as a single layer net. The class needs instructions on what to do when it's created and this is the `__init__` dunder method. With a single 'layer' inside it which makes use of the `Linear` module from PyTorch. The `Linear` module will return probabilities for each dimension in the vector that we provide. This just means that if we give it 2 things to compare against, it'll give us the probability of either one. Next up we have the forward method, which returns the output of the model.\n",
    "\n",
    "The next two methods, `training_step` and `validation_step` are what we'll be using for each respective step in the project process later on. `training_step` evaluates the given batch through the model and calculates the cross entropy loss for said batch. Cross entropy loss is the predicted probability compared to how far that is from the actual value.\n",
    "\n",
    "`validation_step` repeats the same as `training_step` but also calculates the accuracy of the model by the count of correct predictions divided by the number of predictions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dimension, output_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, targets.long())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, targets.long())\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        # Calculate the number of correct predictions over the number of predictions\n",
    "        accuracy = torch.tensor(torch.sum(pred==targets).item()/len(pred))\n",
    "        # Detached is used here so the values aren't included in the gradient calculation\n",
    "        return [loss.detach(), accuracy.detach()] \n"
   ]
  },
  {
   "source": [
    "Now we need a way to:\n",
    "\n",
    "1. Evaluate how well our model is performing on a dataset\n",
    "2. Train the model on a dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    outputs = [model.validation_step(batch) for batch in loader]\n",
    "    outputs = torch.tensor(outputs).T\n",
    "    loss, accuracy = torch.mean(outputs, dim=1)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "source": [
    "We fit the model by using the Adam optimizer (which could be replaced with something like gradient descent or otherwise), for a number of epochs (rounds) over the batches in the data. For each epoch, the losses from each batch are used to calculate the gradient, which in turn gets fed into the optimizer to tune the model parameters before moving onto the next batch. To ensure gradients aren't being messed up, we reset them before each batch and then we validate how well our model is performing on the validation set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, epochs, learning_rate, optimizer_function = torch.optim.Adam):\n",
    "    history = {\"loss\" : [], \"accuracy\" : []}\n",
    "    optimizer = optimizer_function(model.parameters(), learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch \", epoch)\n",
    "        #Train\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        #Validate\n",
    "        for batch in val_loader:\n",
    "            loss, accuracy = evaluate(model, val_loader)\n",
    "        print(\"loss: \", loss.item(), \"accuracy: \", accuracy.item(), \"\\n\")\n",
    "        history[\"loss\"].append(loss.item())\n",
    "        history[\"accuracy\"].append(accuracy.item())\n",
    "         \n",
    "    return history"
   ]
  },
  {
   "source": [
    "At last we can train the model and see how well it performs on the test set!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch  0\n",
      "loss:  1.3733323812484741 accuracy:  0.46666666865348816 \n",
      "\n",
      "Epoch  1\n",
      "loss:  1.1744035482406616 accuracy:  0.46666666865348816 \n",
      "\n",
      "Epoch  2\n",
      "loss:  1.032265067100525 accuracy:  0.5 \n",
      "\n",
      "Epoch  3\n",
      "loss:  0.9222539067268372 accuracy:  0.5333333611488342 \n",
      "\n",
      "Epoch  4\n",
      "loss:  0.857440173625946 accuracy:  0.46666666865348816 \n",
      "\n",
      "Epoch  5\n",
      "loss:  0.856093168258667 accuracy:  0.46666666865348816 \n",
      "\n",
      "Epoch  6\n",
      "loss:  0.9064297080039978 accuracy:  0.5 \n",
      "\n",
      "Epoch  7\n",
      "loss:  0.9493064880371094 accuracy:  0.3333333432674408 \n",
      "\n",
      "Epoch  8\n",
      "loss:  0.937039315700531 accuracy:  0.3333333432674408 \n",
      "\n",
      "Epoch  9\n",
      "loss:  0.9049575924873352 accuracy:  0.36666667461395264 \n",
      "\n",
      "Epoch  10\n",
      "loss:  0.866532027721405 accuracy:  0.4000000059604645 \n",
      "\n",
      "Epoch  11\n",
      "loss:  0.8370420932769775 accuracy:  0.5 \n",
      "\n",
      "Epoch  12\n",
      "loss:  0.8083135485649109 accuracy:  0.5333333611488342 \n",
      "\n",
      "Epoch  13\n",
      "loss:  0.7867664694786072 accuracy:  0.6000000238418579 \n",
      "\n",
      "Epoch  14\n",
      "loss:  0.7874231934547424 accuracy:  0.6000000238418579 \n",
      "\n",
      "Epoch  15\n",
      "loss:  0.7854987978935242 accuracy:  0.6000000238418579 \n",
      "\n",
      "Epoch  16\n",
      "loss:  0.7603756189346313 accuracy:  0.7666666507720947 \n",
      "\n",
      "Epoch  17\n",
      "loss:  0.7459529638290405 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  18\n",
      "loss:  0.7309216856956482 accuracy:  0.8333333134651184 \n",
      "\n",
      "Epoch  19\n",
      "loss:  0.7397978901863098 accuracy:  0.7666666507720947 \n",
      "\n",
      "Epoch  20\n",
      "loss:  0.7362257838249207 accuracy:  0.699999988079071 \n",
      "\n",
      "Epoch  21\n",
      "loss:  0.721867561340332 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  22\n",
      "loss:  0.7000483274459839 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  23\n",
      "loss:  0.6850998997688293 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  24\n",
      "loss:  0.674744725227356 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  25\n",
      "loss:  0.6608838438987732 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  26\n",
      "loss:  0.657321035861969 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  27\n",
      "loss:  0.652073860168457 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  28\n",
      "loss:  0.6370083093643188 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  29\n",
      "loss:  0.6409473419189453 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  30\n",
      "loss:  0.6353674530982971 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  31\n",
      "loss:  0.6202357411384583 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  32\n",
      "loss:  0.6090561151504517 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  33\n",
      "loss:  0.616327166557312 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  34\n",
      "loss:  0.6013180017471313 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  35\n",
      "loss:  0.5989049077033997 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  36\n",
      "loss:  0.585862398147583 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  37\n",
      "loss:  0.5751062035560608 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  38\n",
      "loss:  0.5739135146141052 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  39\n",
      "loss:  0.5716326832771301 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  40\n",
      "loss:  0.5674498677253723 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  41\n",
      "loss:  0.5521577596664429 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  42\n",
      "loss:  0.5762258172035217 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  43\n",
      "loss:  0.5774533748626709 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  44\n",
      "loss:  0.5612514019012451 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  45\n",
      "loss:  0.5368048548698425 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  46\n",
      "loss:  0.5354655981063843 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  47\n",
      "loss:  0.5438606142997742 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  48\n",
      "loss:  0.5306044816970825 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  49\n",
      "loss:  0.5160161852836609 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  50\n",
      "loss:  0.5150001645088196 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  51\n",
      "loss:  0.5204880237579346 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  52\n",
      "loss:  0.5257148146629333 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  53\n",
      "loss:  0.5144279599189758 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  54\n",
      "loss:  0.5133593678474426 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  55\n",
      "loss:  0.5049706697463989 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  56\n",
      "loss:  0.4992431104183197 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  57\n",
      "loss:  0.48528751730918884 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  58\n",
      "loss:  0.4979083240032196 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  59\n",
      "loss:  0.4917204678058624 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  60\n",
      "loss:  0.4880892336368561 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  61\n",
      "loss:  0.4772476553916931 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  62\n",
      "loss:  0.48232564330101013 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  63\n",
      "loss:  0.4626544415950775 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  64\n",
      "loss:  0.460956871509552 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  65\n",
      "loss:  0.47053277492523193 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  66\n",
      "loss:  0.47045251727104187 accuracy:  1.0 \n",
      "\n",
      "Epoch  67\n",
      "loss:  0.4672728478908539 accuracy:  1.0 \n",
      "\n",
      "Epoch  68\n",
      "loss:  0.46997493505477905 accuracy:  1.0 \n",
      "\n",
      "Epoch  69\n",
      "loss:  0.4581855535507202 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  70\n",
      "loss:  0.45587196946144104 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  71\n",
      "loss:  0.45518845319747925 accuracy:  1.0 \n",
      "\n",
      "Epoch  72\n",
      "loss:  0.43724337220191956 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  73\n",
      "loss:  0.44379836320877075 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  74\n",
      "loss:  0.4456246495246887 accuracy:  1.0 \n",
      "\n",
      "Epoch  75\n",
      "loss:  0.4562903046607971 accuracy:  1.0 \n",
      "\n",
      "Epoch  76\n",
      "loss:  0.440542608499527 accuracy:  1.0 \n",
      "\n",
      "Epoch  77\n",
      "loss:  0.45213446021080017 accuracy:  1.0 \n",
      "\n",
      "Epoch  78\n",
      "loss:  0.44089940190315247 accuracy:  1.0 \n",
      "\n",
      "Epoch  79\n",
      "loss:  0.4321136474609375 accuracy:  1.0 \n",
      "\n",
      "Epoch  80\n",
      "loss:  0.4313907027244568 accuracy:  1.0 \n",
      "\n",
      "Epoch  81\n",
      "loss:  0.4239611327648163 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  82\n",
      "loss:  0.4376215636730194 accuracy:  1.0 \n",
      "\n",
      "Epoch  83\n",
      "loss:  0.42454618215560913 accuracy:  1.0 \n",
      "\n",
      "Epoch  84\n",
      "loss:  0.4141182601451874 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  85\n",
      "loss:  0.4074815809726715 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  86\n",
      "loss:  0.42338821291923523 accuracy:  1.0 \n",
      "\n",
      "Epoch  87\n",
      "loss:  0.43143796920776367 accuracy:  1.0 \n",
      "\n",
      "Epoch  88\n",
      "loss:  0.41920754313468933 accuracy:  1.0 \n",
      "\n",
      "Epoch  89\n",
      "loss:  0.4175216555595398 accuracy:  1.0 \n",
      "\n",
      "Epoch  90\n",
      "loss:  0.3996005356311798 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  91\n",
      "loss:  0.391110360622406 accuracy:  0.9666666388511658 \n",
      "\n",
      "Epoch  92\n",
      "loss:  0.41220948100090027 accuracy:  1.0 \n",
      "\n",
      "Epoch  93\n",
      "loss:  0.4087773263454437 accuracy:  1.0 \n",
      "\n",
      "Epoch  94\n",
      "loss:  0.40749818086624146 accuracy:  1.0 \n",
      "\n",
      "Epoch  95\n",
      "loss:  0.3955965042114258 accuracy:  1.0 \n",
      "\n",
      "Epoch  96\n",
      "loss:  0.395894855260849 accuracy:  1.0 \n",
      "\n",
      "Epoch  97\n",
      "loss:  0.399051696062088 accuracy:  1.0 \n",
      "\n",
      "Epoch  98\n",
      "loss:  0.39189818501472473 accuracy:  1.0 \n",
      "\n",
      "Epoch  99\n",
      "loss:  0.38788095116615295 accuracy:  1.0 \n",
      "\n",
      "Evaluation result: Loss:  0.3645802438259125  Accuracy:  0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LogisticRegression(len(selected_features), len(species))\n",
    "fit(model, train_loader, val_loader, epochs, learning_rate)\n",
    "loss , accuracy = evaluate(model, test_loader)\n",
    "print(\"Evaluation result: Loss: \", loss.item(), \" Accuracy: \", accuracy.item())"
   ]
  },
  {
   "source": [
    "As we can see after 100 epochs, we've got an accuracy of 93.3%! Now let's give it a go if it was using all the features (with no feature selection)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch  0\n",
      "loss:  1.0380855798721313 accuracy:  0.4333333373069763 \n",
      "\n",
      "Epoch  1\n",
      "loss:  0.9261402487754822 accuracy:  0.5 \n",
      "\n",
      "Epoch  2\n",
      "loss:  0.9148039221763611 accuracy:  0.6666666865348816 \n",
      "\n",
      "Epoch  3\n",
      "loss:  0.8054198026657104 accuracy:  0.6000000238418579 \n",
      "\n",
      "Epoch  4\n",
      "loss:  0.7064180374145508 accuracy:  0.5 \n",
      "\n",
      "Epoch  5\n",
      "loss:  0.6529492735862732 accuracy:  0.7666666507720947 \n",
      "\n",
      "Epoch  6\n",
      "loss:  0.6175327301025391 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  7\n",
      "loss:  0.5992363095283508 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  8\n",
      "loss:  0.569818377494812 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  9\n",
      "loss:  0.5336132049560547 accuracy:  0.800000011920929 \n",
      "\n",
      "Epoch  10\n",
      "loss:  0.5291264057159424 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  11\n",
      "loss:  0.5121068358421326 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  12\n",
      "loss:  0.4883791506290436 accuracy:  0.8333333134651184 \n",
      "\n",
      "Epoch  13\n",
      "loss:  0.4751811921596527 accuracy:  0.8333333134651184 \n",
      "\n",
      "Epoch  14\n",
      "loss:  0.47150567173957825 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  15\n",
      "loss:  0.46534866094589233 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  16\n",
      "loss:  0.46108758449554443 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  17\n",
      "loss:  0.4389108121395111 accuracy:  0.8333333134651184 \n",
      "\n",
      "Epoch  18\n",
      "loss:  0.43451133370399475 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  19\n",
      "loss:  0.43520790338516235 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  20\n",
      "loss:  0.4287680685520172 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  21\n",
      "loss:  0.4153631925582886 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  22\n",
      "loss:  0.4099046587944031 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  23\n",
      "loss:  0.41186949610710144 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  24\n",
      "loss:  0.40758344531059265 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  25\n",
      "loss:  0.40735262632369995 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  26\n",
      "loss:  0.38762298226356506 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  27\n",
      "loss:  0.3855837285518646 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  28\n",
      "loss:  0.3935543894767761 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  29\n",
      "loss:  0.385096937417984 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  30\n",
      "loss:  0.3809642493724823 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  31\n",
      "loss:  0.36828091740608215 accuracy:  0.8666666746139526 \n",
      "\n",
      "Epoch  32\n",
      "loss:  0.3706659972667694 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  33\n",
      "loss:  0.3665415346622467 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  34\n",
      "loss:  0.36496639251708984 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  35\n",
      "loss:  0.3568505048751831 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  36\n",
      "loss:  0.3588642179965973 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  37\n",
      "loss:  0.3553452789783478 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  38\n",
      "loss:  0.34953799843788147 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  39\n",
      "loss:  0.3425033688545227 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  40\n",
      "loss:  0.34755852818489075 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  41\n",
      "loss:  0.34882432222366333 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  42\n",
      "loss:  0.33427709341049194 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  43\n",
      "loss:  0.3302716612815857 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  44\n",
      "loss:  0.3407600522041321 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  45\n",
      "loss:  0.33250272274017334 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  46\n",
      "loss:  0.3252996802330017 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  47\n",
      "loss:  0.32019010186195374 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  48\n",
      "loss:  0.3239607512950897 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  49\n",
      "loss:  0.32058006525039673 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  50\n",
      "loss:  0.3138344883918762 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  51\n",
      "loss:  0.3136472702026367 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  52\n",
      "loss:  0.3152977228164673 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  53\n",
      "loss:  0.3074900507926941 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  54\n",
      "loss:  0.3046126961708069 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  55\n",
      "loss:  0.3030826151371002 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  56\n",
      "loss:  0.30254852771759033 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  57\n",
      "loss:  0.29916849732398987 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  58\n",
      "loss:  0.30322736501693726 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  59\n",
      "loss:  0.2978363633155823 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  60\n",
      "loss:  0.2937186360359192 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  61\n",
      "loss:  0.2903176546096802 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  62\n",
      "loss:  0.2933221757411957 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  63\n",
      "loss:  0.29069578647613525 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  64\n",
      "loss:  0.2844267785549164 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  65\n",
      "loss:  0.2835501730442047 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  66\n",
      "loss:  0.28674691915512085 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  67\n",
      "loss:  0.2814694344997406 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  68\n",
      "loss:  0.27780818939208984 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  69\n",
      "loss:  0.2762349247932434 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  70\n",
      "loss:  0.2745935618877411 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  71\n",
      "loss:  0.2776546776294708 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  72\n",
      "loss:  0.27241477370262146 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  73\n",
      "loss:  0.26993533968925476 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  74\n",
      "loss:  0.2688852846622467 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  75\n",
      "loss:  0.26922425627708435 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  76\n",
      "loss:  0.2651296854019165 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  77\n",
      "loss:  0.26544901728630066 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  78\n",
      "loss:  0.26239722967147827 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  79\n",
      "loss:  0.26078179478645325 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  80\n",
      "loss:  0.26017817854881287 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  81\n",
      "loss:  0.259433776140213 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  82\n",
      "loss:  0.2574969530105591 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  83\n",
      "loss:  0.25558483600616455 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  84\n",
      "loss:  0.2546296715736389 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  85\n",
      "loss:  0.2527022659778595 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  86\n",
      "loss:  0.2516855299472809 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  87\n",
      "loss:  0.2514974772930145 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  88\n",
      "loss:  0.2494964450597763 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  89\n",
      "loss:  0.25158631801605225 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  90\n",
      "loss:  0.246656596660614 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  91\n",
      "loss:  0.24590688943862915 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  92\n",
      "loss:  0.24534302949905396 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  93\n",
      "loss:  0.24302555620670319 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  94\n",
      "loss:  0.2425689548254013 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  95\n",
      "loss:  0.24102306365966797 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  96\n",
      "loss:  0.23951323330402374 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  97\n",
      "loss:  0.23847466707229614 accuracy:  0.9333333373069763 \n",
      "\n",
      "Epoch  98\n",
      "loss:  0.23755855858325958 accuracy:  0.8999999761581421 \n",
      "\n",
      "Epoch  99\n",
      "loss:  0.23631232976913452 accuracy:  0.8999999761581421 \n",
      "\n",
      "Evaluation result: Loss:  0.16669009625911713  Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "data_all = torch.utils.data.TensorDataset(input_columns_all, output_columns)\n",
    "\n",
    "#train_split, val_split and test_split defined earlier\n",
    "train_set_all, val_set_all, test_set_all = torch.utils.data.random_split(data_all, [train_split, val_split, test_split])\n",
    "\n",
    "train_loader_all = torch.utils.data.DataLoader(train_set_all, 16, shuffle = True)\n",
    "val_loader_all = torch.utils.data.DataLoader(val_set_all)\n",
    "test_loader_all = torch.utils.data.DataLoader(test_set_all)\n",
    "\n",
    "model_all = LogisticRegression(4, len(species))\n",
    "history_all = fit(model_all, train_loader_all, val_loader_all, epochs, learning_rate)\n",
    "loss , accuracy = evaluate(model_all, test_loader_all)\n",
    "print(\"Evaluation result: Loss: \", loss.item(), \" Accuracy: \", accuracy.item())"
   ]
  },
  {
   "source": [
    "Wow! Using all the features got us 100% accuracy on the test set, this doesn't mean that always using all features will lead to this. It's a combination of balancing batch size, learning rates, epochs and more to get the right model for a situation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}